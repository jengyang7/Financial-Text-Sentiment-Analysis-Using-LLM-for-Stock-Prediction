{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3-8B Comparison Across Agreement Levels\n",
    "\n",
    "This notebook fine-tunes Llama 3-8B using different agreement levels from FinancialPhraseBank to compare the impact of data quality on model performance.\n",
    "\n",
    "## Agreement Levels:\n",
    "- **sentences_50agree**: >=50% annotator agreement (4,846 sentences)\n",
    "- **sentences_66agree**: >=66% annotator agreement (4,217 sentences) \n",
    "- **sentences_75agree**: >=75% annotator agreement (3,453 sentences)\n",
    "- **sentences_allagree**: 100% annotator agreement (2,264 sentences)\n",
    "\n",
    "## Hypothesis:\n",
    "Higher agreement levels should lead to better model performance due to reduced label noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorboard (c:\\Users\\Jayden\\.venvs\\fyp-venv\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
    "%pip install -q -U transformers==4.38.2\n",
    "%pip install -q accelerate==0.32.0\n",
    "%pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "%pip install -q -U datasets==2.16.1\n",
    "%pip install -q -U trl==0.7.11\n",
    "%pip install -q -U peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.38.2\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments, \n",
    "    pipeline, \n",
    "    logging\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"transformers=={transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    \"\"\"Clear GPU and CPU memory\"\"\"\n",
    "    print(\"Clearing memory...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "\n",
    "def wait_and_clear(seconds=60):\n",
    "    \"\"\"Wait for specified seconds and clear memory\"\"\"\n",
    "    print(f\"Waiting {seconds} seconds...\")\n",
    "    time.sleep(seconds)\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(agreement_level):\n",
    "    \"\"\"Load and prepare dataset for specific agreement level\"\"\"\n",
    "    print(f\"\\nLoading dataset: {agreement_level}...\")\n",
    "    \n",
    "    # Load the dataset from HuggingFace\n",
    "    dataset = load_dataset(\"takala/financial_phrasebank\", agreement_level)\n",
    "    \n",
    "    # Convert to pandas for easier manipulation\n",
    "    df = dataset['train'].to_pandas()\n",
    "    df = df.rename(columns={'sentence': 'text', 'label': 'sentiment'})\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Sentiment distribution:\")\n",
    "    sentiment_dist = df['sentiment'].value_counts()\n",
    "    print(sentiment_dist)\n",
    "    \n",
    "    return df, sentiment_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_splits(df, max_samples_per_class=300):\n",
    "    \"\"\"Create balanced train/val/test splits\"\"\"\n",
    "    # Calculate minimum class size\n",
    "    min_class_size = df['sentiment'].value_counts().min()\n",
    "    samples_per_class = min(max_samples_per_class, min_class_size)\n",
    "    \n",
    "    # Use 70% for training, 15% for validation, 15% for testing\n",
    "    train_size_per_class = int(samples_per_class * 0.7)\n",
    "    val_size_per_class = int(samples_per_class * 0.15)\n",
    "    test_size_per_class = samples_per_class - train_size_per_class - val_size_per_class\n",
    "    \n",
    "    print(f\"Split sizes per class: Train={train_size_per_class}, Val={val_size_per_class}, Test={test_size_per_class}\")\n",
    "    \n",
    "    X_train, X_val, X_test = [], [], []\n",
    "    \n",
    "    # Map integer labels to sentiment names\n",
    "    label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    \n",
    "    for sentiment_label in [0, 1, 2]:\n",
    "        sentiment_name = label_mapping[sentiment_label]\n",
    "        sentiment_data = df[df.sentiment == sentiment_label]\n",
    "        \n",
    "        if len(sentiment_data) == 0:\n",
    "            print(f\"Warning: No samples found for {sentiment_name} sentiment\")\n",
    "            continue\n",
    "        \n",
    "        # Sample the required number for this class\n",
    "        if len(sentiment_data) >= samples_per_class:\n",
    "            sampled_data = sentiment_data.sample(n=samples_per_class, random_state=42)\n",
    "        else:\n",
    "            sampled_data = sentiment_data\n",
    "            print(f\"Warning: Only {len(sentiment_data)} samples available for {sentiment_name}\")\n",
    "        \n",
    "        # Split the sampled data\n",
    "        if len(sampled_data) >= 3:  # Need at least 3 samples to split\n",
    "            temp_data, test_data = train_test_split(\n",
    "                sampled_data, \n",
    "                test_size=min(test_size_per_class, len(sampled_data)//3),\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            if len(temp_data) >= 2:\n",
    "                train_data, val_data = train_test_split(\n",
    "                    temp_data,\n",
    "                    test_size=min(val_size_per_class, len(temp_data)//2),\n",
    "                    random_state=42\n",
    "                )\n",
    "            else:\n",
    "                train_data = temp_data\n",
    "                val_data = pd.DataFrame()\n",
    "        else:\n",
    "            train_data = sampled_data\n",
    "            val_data = pd.DataFrame()\n",
    "            test_data = pd.DataFrame()\n",
    "        \n",
    "        if len(train_data) > 0:\n",
    "            X_train.append(train_data)\n",
    "        if len(val_data) > 0:\n",
    "            X_val.append(val_data)\n",
    "        if len(test_data) > 0:\n",
    "            X_test.append(test_data)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    X_train = pd.concat(X_train).sample(frac=1, random_state=10).reset_index(drop=True) if X_train else pd.DataFrame()\n",
    "    X_val = pd.concat(X_val).sample(frac=1, random_state=10).reset_index(drop=True) if X_val else pd.DataFrame()\n",
    "    X_test = pd.concat(X_test).sample(frac=1, random_state=10).reset_index(drop=True) if X_test else pd.DataFrame()\n",
    "    \n",
    "    print(f\"Final splits: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(X_train, X_val, X_test, tokenizer):\n",
    "    \"\"\"Prepare training prompts\"\"\"\n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "    \n",
    "    def generate_prompt(data_point):\n",
    "        sentiment_text = label_mapping[data_point[\"sentiment\"]]\n",
    "        return f\"\"\"Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "                determine if it is positive, neutral, or negative, and return the answer as \n",
    "                the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "                [{data_point[\"text\"]}] = {sentiment_text}\n",
    "                \"\"\".strip() + EOS_TOKEN\n",
    "\n",
    "    def generate_test_prompt(data_point):\n",
    "        return f\"\"\"Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "                determine if it is positive, neutral, or negative, and return the answer as \n",
    "                the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\"\n",
    "\n",
    "                [{data_point[\"text\"]}] = \n",
    "                \"\"\".strip()\n",
    "    \n",
    "    # Generate prompts\n",
    "    train_prompts = pd.DataFrame(X_train.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
    "    val_prompts = pd.DataFrame(X_val.apply(generate_prompt, axis=1), columns=[\"text\"])\n",
    "    \n",
    "    # Test prompts and true labels\n",
    "    y_true = [label_mapping[label] for label in X_test['sentiment'].tolist()]\n",
    "    test_prompts = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "    \n",
    "    # Convert to HuggingFace datasets\n",
    "    train_data = Dataset.from_pandas(train_prompts)\n",
    "    eval_data = Dataset.from_pandas(val_prompts)\n",
    "    \n",
    "    return train_data, eval_data, test_prompts, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load fresh model and tokenizer\"\"\"\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "    \n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config, \n",
    "        token=True  # Add this for accessing gated models\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    # Load tokenizer\n",
    "    max_seq_length = 2048\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length, token=True)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, agreement_level):\n",
    "    \"\"\"Comprehensive evaluation function\"\"\"\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none': 1, 'negative': 0}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS FOR {agreement_level.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    unique_labels = set(y_true_mapped)\n",
    "    \n",
    "    class_accuracies = {}\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) \n",
    "                         if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        label_name = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}[label]\n",
    "        class_accuracies[label_name] = label_accuracy\n",
    "        print(f'{label_name} Accuracy: {label_accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped,\n",
    "        target_names=['Negative', 'Neutral', 'Positive'],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(\n",
    "        y_true=y_true_mapped, \n",
    "        y_pred=y_pred_mapped,\n",
    "        target_names=['Negative', 'Neutral', 'Positive']\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'agreement_level': agreement_level,\n",
    "        'overall_accuracy': accuracy,\n",
    "        'class_accuracies': class_accuracies,\n",
    "        'classification_report': class_report,\n",
    "        'test_samples': len(y_true)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(test_prompts, model, tokenizer):\n",
    "    \"\"\"Prediction function\"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(test_prompts)), desc=\"Predicting\"):\n",
    "        prompt = test_prompts.iloc[i][\"text\"]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **input_ids, \n",
    "                max_new_tokens=1, \n",
    "                temperature=0.0,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        result = tokenizer.decode(outputs[0])\n",
    "        answer = result.split(\"=\")[-1].lower().strip()\n",
    "        \n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, tokenizer, train_data, eval_data, agreement_level):\n",
    "    \"\"\"Fine-tune model for specific agreement level\"\"\"\n",
    "    # LoRA configuration for Llama 3\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_steps = len(train_data) // 8\n",
    "    eval_steps = max(training_steps // 10, 10)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=f\"logs_{agreement_level}\",\n",
    "        num_train_epochs=3,  # Reduced epochs for comparison\n",
    "        gradient_checkpointing=True,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=0,\n",
    "        logging_steps=max(training_steps // 20, 5),\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.001,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=False,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=eval_steps,\n",
    "        eval_accumulation_steps=1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=2048,\n",
    "        args=training_arguments,\n",
    "        packing=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStarting fine-tuning for {agreement_level}...\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(eval_data)}\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model\n",
    "    model_save_path = f\"../models/trained-llama3-{agreement_level}\"\n",
    "    trainer.model.save_pretrained(model_save_path)\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "    \n",
    "    return trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Comparison Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comparison across 4 agreement levels...\n",
      "Agreement levels: ['sentences_50agree', 'sentences_66agree', 'sentences_75agree', 'sentences_allagree']\n",
      "Clearing memory...\n",
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Define agreement levels to test\n",
    "agreement_levels = [\n",
    "    \"sentences_50agree\",\n",
    "    \"sentences_66agree\", \n",
    "    \"sentences_75agree\",\n",
    "    \"sentences_allagree\"\n",
    "]\n",
    "\n",
    "# Store results\n",
    "all_results = []\n",
    "detailed_results = {}\n",
    "\n",
    "print(f\"Starting comparison across {len(agreement_levels)} agreement levels...\")\n",
    "print(f\"Agreement levels: {agreement_levels}\")\n",
    "\n",
    "# Clear initial memory\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING SENTENCES_50AGREE (1/4)\n",
      "================================================================================\n",
      "\n",
      "Loading dataset: sentences_50agree...\n",
      "Dataset shape: (4846, 2)\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    2879\n",
      "2    1363\n",
      "0     604\n",
      "Name: count, dtype: int64\n",
      "Split sizes per class: Train=210, Val=45, Test=45\n",
      "Final splits: Train=630, Val=135, Test=135\n",
      "\n",
      "Loading fresh model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fb7e44ba21454da43af2f267035a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing baseline performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 1/20 [00:02<00:50,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 2/20 [00:02<00:21,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▌        | 3/20 [00:02<00:12,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 4/20 [00:03<00:07,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 5/20 [00:03<00:05,  2.73it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 6/20 [00:03<00:04,  3.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▌      | 7/20 [00:03<00:03,  4.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 8/20 [00:03<00:02,  4.66it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 9/20 [00:03<00:02,  5.14it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 10/20 [00:04<00:01,  5.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▌    | 11/20 [00:04<00:01,  5.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 12/20 [00:04<00:01,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 13/20 [00:04<00:01,  6.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 14/20 [00:04<00:00,  6.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▌  | 15/20 [00:04<00:00,  6.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 16/20 [00:04<00:00,  6.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 17/20 [00:05<00:00,  6.58it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 18/20 [00:05<00:00,  6.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▌| 19/20 [00:05<00:00,  6.61it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 20/20 [00:05<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on 20 samples: 0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e4cb77e50342e680181d2a60c7bcac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87ef1ce16aa4706b73a07fdc0f0733b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning for sentences_50agree...\n",
      "Training samples: 630\n",
      "Validation samples: 135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691d54763d784bd198fbd561e6afac25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9091, 'grad_norm': 0.5394439101219177, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 1.7832, 'grad_norm': 0.9753520488739014, 'learning_rate': 0.00019996135574945544, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcdcf819c164249aede0a26fef31c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2524452209472656, 'eval_runtime': 49.154, 'eval_samples_per_second': 2.746, 'eval_steps_per_second': 0.346, 'epoch': 0.13}\n",
      "{'loss': 1.142, 'grad_norm': 0.3727717697620392, 'learning_rate': 0.00019952695086820975, 'epoch': 0.19}\n",
      "{'loss': 1.0257, 'grad_norm': 0.5527883172035217, 'learning_rate': 0.00019861194048993863, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa51fd437a1494e8c6f7d7d69075e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0498992204666138, 'eval_runtime': 45.9934, 'eval_samples_per_second': 2.935, 'eval_steps_per_second': 0.37, 'epoch': 0.25}\n",
      "{'loss': 1.0193, 'grad_norm': 0.4222785234451294, 'learning_rate': 0.00019722074310645553, 'epoch': 0.32}\n",
      "{'loss': 0.9567, 'grad_norm': 0.32622280716896057, 'learning_rate': 0.00019536007666806556, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23a506d3e0e4e4aa20d395286972dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9793999791145325, 'eval_runtime': 46.9093, 'eval_samples_per_second': 2.878, 'eval_steps_per_second': 0.362, 'epoch': 0.38}\n",
      "{'loss': 0.9352, 'grad_norm': 0.3624098002910614, 'learning_rate': 0.00019303892614326836, 'epoch': 0.44}\n",
      "{'loss': 0.9384, 'grad_norm': 0.27643492817878723, 'learning_rate': 0.00019026850013126157, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9f5bc476b94e1dbe348e8fdc3ad12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9562624096870422, 'eval_runtime': 47.1221, 'eval_samples_per_second': 2.865, 'eval_steps_per_second': 0.361, 'epoch': 0.51}\n",
      "{'loss': 0.9114, 'grad_norm': 0.25675663352012634, 'learning_rate': 0.00018706217673675811, 'epoch': 0.57}\n",
      "{'loss': 0.791, 'grad_norm': 0.24408505856990814, 'learning_rate': 0.00018343543896848273, 'epoch': 0.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc7c76e8fa44cbfab7e6a03377074d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9446445107460022, 'eval_runtime': 46.8777, 'eval_samples_per_second': 2.88, 'eval_steps_per_second': 0.363, 'epoch': 0.63}\n",
      "{'loss': 0.886, 'grad_norm': 0.28039151430130005, 'learning_rate': 0.00017940579997330165, 'epoch': 0.7}\n",
      "{'loss': 0.9059, 'grad_norm': 0.32270902395248413, 'learning_rate': 0.00017499271846702213, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1fb60cd3ab4129b89223dbe49517e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9350346326828003, 'eval_runtime': 47.7045, 'eval_samples_per_second': 2.83, 'eval_steps_per_second': 0.356, 'epoch': 0.76}\n",
      "{'loss': 0.764, 'grad_norm': 0.24870052933692932, 'learning_rate': 0.0001702175047702382, 'epoch': 0.83}\n",
      "{'loss': 0.8961, 'grad_norm': 0.4486890435218811, 'learning_rate': 0.00016510321790296525, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11424e2e323e4098a5af86d8d235daff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9253931045532227, 'eval_runtime': 47.1801, 'eval_samples_per_second': 2.861, 'eval_steps_per_second': 0.36, 'epoch': 0.89}\n",
      "{'loss': 0.8928, 'grad_norm': 0.3704843521118164, 'learning_rate': 0.00015967455423498387, 'epoch': 0.95}\n",
      "{'loss': 0.7744, 'grad_norm': 0.24348542094230652, 'learning_rate': 0.00015395772822958845, 'epoch': 1.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ae6ec654f45c3920ac6858509de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.916245698928833, 'eval_runtime': 47.4302, 'eval_samples_per_second': 2.846, 'eval_steps_per_second': 0.358, 'epoch': 1.02}\n",
      "{'loss': 0.6611, 'grad_norm': 0.2870008051395416, 'learning_rate': 0.00014798034585661695, 'epoch': 1.08}\n",
      "{'loss': 0.7261, 'grad_norm': 0.28376466035842896, 'learning_rate': 0.00014177127128603745, 'epoch': 1.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea1b5ac4b1741c9a7b8a1ba2303941d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9374059438705444, 'eval_runtime': 47.502, 'eval_samples_per_second': 2.842, 'eval_steps_per_second': 0.358, 'epoch': 1.14}\n",
      "{'loss': 0.6997, 'grad_norm': 0.26925286650657654, 'learning_rate': 0.00013536048750581494, 'epoch': 1.21}\n",
      "{'loss': 0.6596, 'grad_norm': 0.22904078662395477, 'learning_rate': 0.00012877895153711935, 'epoch': 1.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e86a22dcc947899e37b54b9ffe7792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9309154748916626, 'eval_runtime': 47.5794, 'eval_samples_per_second': 2.837, 'eval_steps_per_second': 0.357, 'epoch': 1.27}\n",
      "{'loss': 0.7472, 'grad_norm': 0.3242730498313904, 'learning_rate': 0.0001220584449460274, 'epoch': 1.33}\n",
      "{'loss': 0.8373, 'grad_norm': 0.4210909903049469, 'learning_rate': 0.0001152314203735805, 'epoch': 1.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee38f9214a845509cc3d1c3c50704d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9469806551933289, 'eval_runtime': 46.642, 'eval_samples_per_second': 2.894, 'eval_steps_per_second': 0.364, 'epoch': 1.4}\n",
      "{'loss': 0.6988, 'grad_norm': 0.39314207434654236, 'learning_rate': 0.00010833084482529048, 'epoch': 1.46}\n",
      "{'loss': 0.6876, 'grad_norm': 0.2516789734363556, 'learning_rate': 0.00010139004047683151, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3ff660efce42a4983bb101249c6329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9292834401130676, 'eval_runtime': 46.6709, 'eval_samples_per_second': 2.893, 'eval_steps_per_second': 0.364, 'epoch': 1.52}\n",
      "{'loss': 0.7352, 'grad_norm': 0.3526500463485718, 'learning_rate': 9.444252376465171e-05, 'epoch': 1.59}\n",
      "{'loss': 0.631, 'grad_norm': 0.3447439968585968, 'learning_rate': 8.752184353851916e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7932b3aeda648f4af4416d2376a117a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9360257983207703, 'eval_runtime': 47.9472, 'eval_samples_per_second': 2.816, 'eval_steps_per_second': 0.355, 'epoch': 1.65}\n",
      "{'loss': 0.7201, 'grad_norm': 0.3587779700756073, 'learning_rate': 8.066141905754723e-05, 'epoch': 1.71}\n",
      "{'loss': 0.6421, 'grad_norm': 0.2924353778362274, 'learning_rate': 7.389437861200024e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933b0295d0fb4306a3f4702a7bee2ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9335948824882507, 'eval_runtime': 47.6309, 'eval_samples_per_second': 2.834, 'eval_steps_per_second': 0.357, 'epoch': 1.78}\n",
      "{'loss': 0.8031, 'grad_norm': 0.31765973567962646, 'learning_rate': 6.725339955015777e-05, 'epoch': 1.84}\n",
      "{'loss': 0.698, 'grad_norm': 0.38972336053848267, 'learning_rate': 6.0770550482731924e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b8345d251948f38846a9b831656b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9342652559280396, 'eval_runtime': 46.1723, 'eval_samples_per_second': 2.924, 'eval_steps_per_second': 0.368, 'epoch': 1.9}\n",
      "{'loss': 0.6709, 'grad_norm': 0.35808658599853516, 'learning_rate': 5.447713642681612e-05, 'epoch': 1.97}\n",
      "{'loss': 0.6912, 'grad_norm': 0.33790621161460876, 'learning_rate': 4.840354763714991e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb27850f319a4dd7b9e0baa188974d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9364230632781982, 'eval_runtime': 47.6642, 'eval_samples_per_second': 2.832, 'eval_steps_per_second': 0.357, 'epoch': 2.03}\n",
      "{'loss': 0.5707, 'grad_norm': 0.31081220507621765, 'learning_rate': 4.257911285467754e-05, 'epoch': 2.1}\n",
      "{'loss': 0.5257, 'grad_norm': 0.3687444031238556, 'learning_rate': 3.7031957681048604e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef31899368b94f1d9d4afd6406b8c130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9695270657539368, 'eval_runtime': 46.0496, 'eval_samples_per_second': 2.932, 'eval_steps_per_second': 0.369, 'epoch': 2.16}\n",
      "{'loss': 0.5245, 'grad_norm': 0.3477259576320648, 'learning_rate': 3.178886876295578e-05, 'epoch': 2.22}\n",
      "{'loss': 0.5052, 'grad_norm': 0.409900039434433, 'learning_rate': 2.6875164442149147e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8890f7d82b9743a6ba2d9dace69ddfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.001775860786438, 'eval_runtime': 45.8078, 'eval_samples_per_second': 2.947, 'eval_steps_per_second': 0.371, 'epoch': 2.29}\n",
      "{'loss': 0.5545, 'grad_norm': 0.5080394744873047, 'learning_rate': 2.2314572495745746e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4804, 'grad_norm': 0.4507872760295868, 'learning_rate': 1.8129115557213262e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c358adac465142fc8dde77361603f0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0013407468795776, 'eval_runtime': 45.9395, 'eval_samples_per_second': 2.939, 'eval_steps_per_second': 0.37, 'epoch': 2.41}\n",
      "{'loss': 0.4976, 'grad_norm': 0.4272882640361786, 'learning_rate': 1.433900477131882e-05, 'epoch': 2.48}\n",
      "{'loss': 0.5167, 'grad_norm': 0.4582081735134125, 'learning_rate': 1.0962542196571634e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826238b1b33d44b68ee778ee89063d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.00126051902771, 'eval_runtime': 46.3291, 'eval_samples_per_second': 2.914, 'eval_steps_per_second': 0.367, 'epoch': 2.54}\n",
      "{'loss': 0.536, 'grad_norm': 0.6060853600502014, 'learning_rate': 8.016032426448817e-06, 'epoch': 2.6}\n",
      "{'loss': 0.5516, 'grad_norm': 0.5654637217521667, 'learning_rate': 5.5137038561761115e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf1de1b708b48f4b351313b2c8f344a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.997919499874115, 'eval_runtime': 45.9686, 'eval_samples_per_second': 2.937, 'eval_steps_per_second': 0.37, 'epoch': 2.67}\n",
      "{'loss': 0.5292, 'grad_norm': 0.5464028716087341, 'learning_rate': 3.467639975257997e-06, 'epoch': 2.73}\n",
      "{'loss': 0.5483, 'grad_norm': 0.44941914081573486, 'learning_rate': 1.88772101753929e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e7cddf121b4f66bdaa7886bb4e6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.99681556224823, 'eval_runtime': 46.1454, 'eval_samples_per_second': 2.926, 'eval_steps_per_second': 0.368, 'epoch': 2.79}\n",
      "{'loss': 0.5345, 'grad_norm': 0.4495753347873688, 'learning_rate': 7.815762505632096e-07, 'epoch': 2.86}\n",
      "{'loss': 0.5099, 'grad_norm': 0.48617178201675415, 'learning_rate': 1.545471346164007e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc80c90ae5144ec684118a16bec28391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9966216087341309, 'eval_runtime': 46.1799, 'eval_samples_per_second': 2.923, 'eval_steps_per_second': 0.368, 'epoch': 2.92}\n",
      "{'train_runtime': 2465.3032, 'train_samples_per_second': 0.767, 'train_steps_per_second': 0.095, 'train_loss': 0.7819739845063951, 'epoch': 2.97}\n",
      "Model saved to: ../models/trained-llama3-sentences_50agree\n",
      "\n",
      "Evaluating fine-tuned model on 135 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/135 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Predicting:   1%|          | 1/135 [00:00<00:29,  4.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|▏         | 2/135 [00:00<00:25,  5.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   2%|▏         | 3/135 [00:00<00:24,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   3%|▎         | 4/135 [00:00<00:22,  5.74it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▎         | 5/135 [00:00<00:22,  5.75it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▍         | 6/135 [00:01<00:22,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 7/135 [00:01<00:22,  5.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   6%|▌         | 8/135 [00:01<00:21,  5.79it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 9/135 [00:01<00:21,  5.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 10/135 [00:01<00:21,  5.75it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   8%|▊         | 11/135 [00:01<00:21,  5.74it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   9%|▉         | 12/135 [00:02<00:20,  5.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|▉         | 13/135 [00:02<00:20,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 14/135 [00:02<00:20,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  11%|█         | 15/135 [00:02<00:20,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  12%|█▏        | 16/135 [00:02<00:20,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 17/135 [00:02<00:19,  5.96it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 18/135 [00:03<00:19,  5.90it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  14%|█▍        | 19/135 [00:03<00:19,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▍        | 20/135 [00:03<00:19,  5.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▌        | 21/135 [00:03<00:19,  5.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▋        | 22/135 [00:03<00:19,  5.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  17%|█▋        | 23/135 [00:04<00:20,  5.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  18%|█▊        | 24/135 [00:04<00:20,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▊        | 25/135 [00:04<00:19,  5.67it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▉        | 26/135 [00:04<00:19,  5.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 27/135 [00:04<00:19,  5.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██        | 28/135 [00:04<00:18,  5.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██▏       | 29/135 [00:05<00:18,  5.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  22%|██▏       | 30/135 [00:05<00:17,  5.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  23%|██▎       | 31/135 [00:05<00:17,  5.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▎       | 32/135 [00:05<00:17,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▍       | 33/135 [00:05<00:17,  5.83it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 34/135 [00:05<00:16,  5.96it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  26%|██▌       | 35/135 [00:06<00:16,  5.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 36/135 [00:06<00:16,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 37/135 [00:06<00:16,  6.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  28%|██▊       | 38/135 [00:06<00:16,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  29%|██▉       | 39/135 [00:06<00:15,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|██▉       | 40/135 [00:06<00:16,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 41/135 [00:07<00:16,  5.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  31%|███       | 42/135 [00:07<00:16,  5.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  32%|███▏      | 43/135 [00:07<00:15,  5.94it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 44/135 [00:07<00:15,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 45/135 [00:07<00:15,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  34%|███▍      | 46/135 [00:07<00:15,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▍      | 47/135 [00:08<00:15,  5.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▌      | 48/135 [00:08<00:14,  5.94it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▋      | 49/135 [00:08<00:14,  5.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  37%|███▋      | 50/135 [00:08<00:14,  5.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  38%|███▊      | 51/135 [00:08<00:14,  5.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▊      | 52/135 [00:08<00:14,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▉      | 53/135 [00:09<00:14,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 54/135 [00:09<00:13,  5.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████      | 55/135 [00:09<00:13,  5.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████▏     | 56/135 [00:09<00:13,  5.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  42%|████▏     | 57/135 [00:09<00:13,  5.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  43%|████▎     | 58/135 [00:09<00:12,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▎     | 59/135 [00:10<00:13,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▍     | 60/135 [00:10<00:12,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 61/135 [00:10<00:12,  6.02it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  46%|████▌     | 62/135 [00:10<00:12,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 63/135 [00:10<00:12,  5.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 64/135 [00:11<00:12,  5.83it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  48%|████▊     | 65/135 [00:11<00:12,  5.81it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  49%|████▉     | 66/135 [00:11<00:11,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|████▉     | 67/135 [00:11<00:11,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 68/135 [00:11<00:11,  5.94it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  51%|█████     | 69/135 [00:11<00:11,  5.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  52%|█████▏    | 70/135 [00:12<00:11,  5.79it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 71/135 [00:12<00:10,  5.94it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 72/135 [00:12<00:10,  5.81it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  54%|█████▍    | 73/135 [00:12<00:10,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▍    | 74/135 [00:12<00:10,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▌    | 75/135 [00:12<00:10,  5.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▋    | 76/135 [00:13<00:10,  5.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  57%|█████▋    | 77/135 [00:13<00:09,  6.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  58%|█████▊    | 78/135 [00:13<00:09,  5.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▊    | 79/135 [00:13<00:09,  6.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▉    | 80/135 [00:13<00:09,  5.94it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 81/135 [00:13<00:09,  5.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████    | 82/135 [00:14<00:09,  5.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████▏   | 83/135 [00:14<00:08,  5.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  62%|██████▏   | 84/135 [00:14<00:08,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  63%|██████▎   | 85/135 [00:14<00:08,  5.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▎   | 86/135 [00:14<00:08,  5.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▍   | 87/135 [00:14<00:08,  5.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 88/135 [00:15<00:07,  5.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  66%|██████▌   | 89/135 [00:15<00:07,  6.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 90/135 [00:15<00:07,  5.96it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 91/135 [00:15<00:07,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  68%|██████▊   | 92/135 [00:15<00:07,  6.02it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  69%|██████▉   | 93/135 [00:15<00:07,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|██████▉   | 94/135 [00:16<00:06,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 95/135 [00:16<00:06,  6.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  71%|███████   | 96/135 [00:16<00:06,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  72%|███████▏  | 97/135 [00:16<00:06,  5.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 98/135 [00:16<00:06,  6.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 99/135 [00:16<00:05,  6.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  74%|███████▍  | 100/135 [00:17<00:05,  5.98it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▍  | 101/135 [00:17<00:05,  6.07it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▌  | 102/135 [00:17<00:05,  5.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▋  | 103/135 [00:17<00:05,  5.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  77%|███████▋  | 104/135 [00:17<00:05,  5.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  78%|███████▊  | 105/135 [00:17<00:05,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▊  | 106/135 [00:18<00:04,  5.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▉  | 107/135 [00:18<00:04,  5.95it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 108/135 [00:18<00:04,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████  | 109/135 [00:18<00:04,  5.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████▏ | 110/135 [00:18<00:04,  6.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  82%|████████▏ | 111/135 [00:18<00:04,  5.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  83%|████████▎ | 112/135 [00:19<00:03,  6.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▎ | 113/135 [00:19<00:03,  6.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▍ | 114/135 [00:19<00:03,  5.86it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 115/135 [00:19<00:03,  5.98it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  86%|████████▌ | 116/135 [00:19<00:03,  5.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 117/135 [00:19<00:03,  5.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 118/135 [00:20<00:02,  5.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  88%|████████▊ | 119/135 [00:20<00:02,  5.90it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  89%|████████▉ | 120/135 [00:20<00:02,  6.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|████████▉ | 121/135 [00:20<00:02,  6.06it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 122/135 [00:20<00:02,  5.96it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  91%|█████████ | 123/135 [00:20<00:02,  5.98it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  92%|█████████▏| 124/135 [00:21<00:01,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 125/135 [00:21<00:01,  5.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 126/135 [00:21<00:01,  5.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  94%|█████████▍| 127/135 [00:21<00:01,  5.92it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▍| 128/135 [00:21<00:01,  5.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▌| 129/135 [00:21<00:01,  5.81it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▋| 130/135 [00:22<00:00,  5.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  97%|█████████▋| 131/135 [00:22<00:00,  5.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  98%|█████████▊| 132/135 [00:22<00:00,  5.71it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▊| 133/135 [00:22<00:00,  5.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▉| 134/135 [00:22<00:00,  5.71it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 135/135 [00:23<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS FOR SENTENCES_50AGREE\n",
      "============================================================\n",
      "Overall Accuracy: 0.874\n",
      "Negative Accuracy: 0.911\n",
      "Neutral Accuracy: 0.844\n",
      "Positive Accuracy: 0.867\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.91      0.92        45\n",
      "     Neutral       0.83      0.84      0.84        45\n",
      "    Positive       0.87      0.87      0.87        45\n",
      "\n",
      "    accuracy                           0.87       135\n",
      "   macro avg       0.87      0.87      0.87       135\n",
      "weighted avg       0.87      0.87      0.87       135\n",
      "\n",
      "\n",
      "Completed sentences_50agree\n",
      "Final accuracy: 0.874\n",
      "Improvement over baseline: 0.374\n",
      "Waiting 60 seconds...\n",
      "Clearing memory...\n",
      "GPU memory allocated: 0.02 GB\n",
      "GPU memory reserved: 1.04 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING SENTENCES_66AGREE (2/4)\n",
      "================================================================================\n",
      "\n",
      "Loading dataset: sentences_66agree...\n",
      "Dataset shape: (4217, 2)\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    2535\n",
      "2    1168\n",
      "0     514\n",
      "Name: count, dtype: int64\n",
      "Split sizes per class: Train=210, Val=45, Test=45\n",
      "Final splits: Train=630, Val=135, Test=135\n",
      "\n",
      "Loading fresh model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4746aaebd1484b568f7b25bc2141d93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing baseline performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 1/20 [00:01<00:23,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 2/20 [00:01<00:10,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▌        | 3/20 [00:01<00:06,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 4/20 [00:01<00:04,  3.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 5/20 [00:01<00:03,  4.07it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 6/20 [00:02<00:03,  4.66it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▌      | 7/20 [00:02<00:02,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 8/20 [00:02<00:02,  5.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 9/20 [00:02<00:01,  5.79it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 10/20 [00:02<00:01,  5.98it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▌    | 11/20 [00:02<00:01,  6.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 12/20 [00:02<00:01,  6.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 13/20 [00:03<00:01,  6.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 14/20 [00:03<00:00,  6.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▌  | 15/20 [00:03<00:00,  6.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 16/20 [00:03<00:00,  6.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 17/20 [00:03<00:00,  6.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 18/20 [00:03<00:00,  6.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▌| 19/20 [00:03<00:00,  6.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 20/20 [00:04<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on 20 samples: 0.400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f680f0201d4c49df9f3985b40adcca73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08944016c9a841eea5e7b6f3838835da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning for sentences_66agree...\n",
      "Training samples: 630\n",
      "Validation samples: 135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21123cfeac44ab6b7032e2fbacd493d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8304, 'grad_norm': 0.5446348190307617, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 1.8879, 'grad_norm': 0.8633529543876648, 'learning_rate': 0.00019996135574945544, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1223699b5042fb95ea216b1f22a718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.156339406967163, 'eval_runtime': 211.2112, 'eval_samples_per_second': 0.639, 'eval_steps_per_second': 0.08, 'epoch': 0.13}\n",
      "{'loss': 1.1505, 'grad_norm': 1.007103443145752, 'learning_rate': 0.00019952695086820975, 'epoch': 0.19}\n",
      "{'loss': 0.9782, 'grad_norm': 0.5200950503349304, 'learning_rate': 0.00019861194048993863, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce2286e534745419bdce039fc877095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9706398248672485, 'eval_runtime': 207.3175, 'eval_samples_per_second': 0.651, 'eval_steps_per_second': 0.082, 'epoch': 0.25}\n",
      "{'loss': 0.9653, 'grad_norm': 0.29955658316612244, 'learning_rate': 0.00019722074310645553, 'epoch': 0.32}\n",
      "{'loss': 0.8947, 'grad_norm': 0.3100181221961975, 'learning_rate': 0.00019536007666806556, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d8ac0bfb284a7bba30e1238adc27d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9062876105308533, 'eval_runtime': 208.7365, 'eval_samples_per_second': 0.647, 'eval_steps_per_second': 0.081, 'epoch': 0.38}\n",
      "{'loss': 0.8886, 'grad_norm': 0.3610437214374542, 'learning_rate': 0.00019303892614326836, 'epoch': 0.44}\n",
      "{'loss': 0.8745, 'grad_norm': 0.30494892597198486, 'learning_rate': 0.00019026850013126157, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d677ffb3b65d4626a2d15e2a9e424964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8732984662055969, 'eval_runtime': 206.9268, 'eval_samples_per_second': 0.652, 'eval_steps_per_second': 0.082, 'epoch': 0.51}\n",
      "{'loss': 0.8472, 'grad_norm': 0.3035907447338104, 'learning_rate': 0.00018706217673675811, 'epoch': 0.57}\n",
      "{'loss': 0.8987, 'grad_norm': 0.3648854196071625, 'learning_rate': 0.00018343543896848273, 'epoch': 0.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a709791c20764d58bc78a12eda5fae4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8642804622650146, 'eval_runtime': 206.648, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 0.63}\n",
      "{'loss': 0.9902, 'grad_norm': 0.313923180103302, 'learning_rate': 0.00017940579997330165, 'epoch': 0.7}\n",
      "{'loss': 0.8262, 'grad_norm': 0.31440868973731995, 'learning_rate': 0.00017499271846702213, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3136bedb9614adf8bd22e63dde45c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8562947511672974, 'eval_runtime': 203.81, 'eval_samples_per_second': 0.662, 'eval_steps_per_second': 0.083, 'epoch': 0.76}\n",
      "{'loss': 0.8269, 'grad_norm': 0.26375895738601685, 'learning_rate': 0.0001702175047702382, 'epoch': 0.83}\n",
      "{'loss': 0.888, 'grad_norm': 0.401833176612854, 'learning_rate': 0.00016510321790296525, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87df1e9bebf49ceafff8bac253b028f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8436304330825806, 'eval_runtime': 207.2318, 'eval_samples_per_second': 0.651, 'eval_steps_per_second': 0.082, 'epoch': 0.89}\n",
      "{'loss': 0.8405, 'grad_norm': 0.30799952149391174, 'learning_rate': 0.00015967455423498387, 'epoch': 0.95}\n",
      "{'loss': 0.7823, 'grad_norm': 0.21458658576011658, 'learning_rate': 0.00015395772822958845, 'epoch': 1.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42483165a88e49ef873741b5bf2303ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8395868539810181, 'eval_runtime': 207.1333, 'eval_samples_per_second': 0.652, 'eval_steps_per_second': 0.082, 'epoch': 1.02}\n",
      "{'loss': 0.7198, 'grad_norm': 0.22920845448970795, 'learning_rate': 0.00014798034585661695, 'epoch': 1.08}\n",
      "{'loss': 0.683, 'grad_norm': 0.2732987105846405, 'learning_rate': 0.00014177127128603745, 'epoch': 1.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87568c6ff28435790f3397fd9a75013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.837647020816803, 'eval_runtime': 206.6152, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 1.14}\n",
      "{'loss': 0.7323, 'grad_norm': 0.3369291126728058, 'learning_rate': 0.00013536048750581494, 'epoch': 1.21}\n",
      "{'loss': 0.7093, 'grad_norm': 0.33015990257263184, 'learning_rate': 0.00012877895153711935, 'epoch': 1.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b197cd0846ce43378036b85d39be6109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.837715744972229, 'eval_runtime': 206.2022, 'eval_samples_per_second': 0.655, 'eval_steps_per_second': 0.082, 'epoch': 1.27}\n",
      "{'loss': 0.6911, 'grad_norm': 0.34088942408561707, 'learning_rate': 0.0001220584449460274, 'epoch': 1.33}\n",
      "{'loss': 0.776, 'grad_norm': 0.3414470851421356, 'learning_rate': 0.0001152314203735805, 'epoch': 1.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3df98b88cd490aadceadadc1f3b6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8460949063301086, 'eval_runtime': 206.3149, 'eval_samples_per_second': 0.654, 'eval_steps_per_second': 0.082, 'epoch': 1.4}\n",
      "{'loss': 0.6992, 'grad_norm': 0.34536927938461304, 'learning_rate': 0.00010833084482529048, 'epoch': 1.46}\n",
      "{'loss': 0.6639, 'grad_norm': 0.38471129536628723, 'learning_rate': 0.00010139004047683151, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7f53f032ba4f79a652b28cc95232da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8463066220283508, 'eval_runtime': 206.579, 'eval_samples_per_second': 0.654, 'eval_steps_per_second': 0.082, 'epoch': 1.52}\n",
      "{'loss': 0.6545, 'grad_norm': 0.36988595128059387, 'learning_rate': 9.444252376465171e-05, 'epoch': 1.59}\n",
      "{'loss': 0.6964, 'grad_norm': 0.34648168087005615, 'learning_rate': 8.752184353851916e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6270b68bd5441479e5b28fa054803c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.841667115688324, 'eval_runtime': 208.1792, 'eval_samples_per_second': 0.648, 'eval_steps_per_second': 0.082, 'epoch': 1.65}\n",
      "{'loss': 0.7604, 'grad_norm': 0.4095156192779541, 'learning_rate': 8.066141905754723e-05, 'epoch': 1.71}\n",
      "{'loss': 0.7227, 'grad_norm': 0.3905446529388428, 'learning_rate': 7.389437861200024e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d50614cd8b45469ba1e54002349aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8395272493362427, 'eval_runtime': 205.3851, 'eval_samples_per_second': 0.657, 'eval_steps_per_second': 0.083, 'epoch': 1.78}\n",
      "{'loss': 0.6974, 'grad_norm': 0.34515321254730225, 'learning_rate': 6.725339955015777e-05, 'epoch': 1.84}\n",
      "{'loss': 0.7026, 'grad_norm': 0.369934618473053, 'learning_rate': 6.0770550482731924e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba372dec60b4a59b5e11be4cfd932e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8392063975334167, 'eval_runtime': 206.7554, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 1.9}\n",
      "{'loss': 0.7357, 'grad_norm': 0.35470104217529297, 'learning_rate': 5.447713642681612e-05, 'epoch': 1.97}\n",
      "{'loss': 0.6057, 'grad_norm': 0.27844318747520447, 'learning_rate': 4.840354763714991e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c565ae246ec04b159f5dd28fc12350a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8451243042945862, 'eval_runtime': 206.7452, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 2.03}\n",
      "{'loss': 0.5628, 'grad_norm': 0.35753053426742554, 'learning_rate': 4.257911285467754e-05, 'epoch': 2.1}\n",
      "{'loss': 0.585, 'grad_norm': 0.4752093553543091, 'learning_rate': 3.7031957681048604e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf527e3bab846f3922359bf3c8494ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8696701526641846, 'eval_runtime': 206.6026, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 2.16}\n",
      "{'loss': 0.5237, 'grad_norm': 0.4945109188556671, 'learning_rate': 3.178886876295578e-05, 'epoch': 2.22}\n",
      "{'loss': 0.5681, 'grad_norm': 0.43065571784973145, 'learning_rate': 2.6875164442149147e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936a65b007c14108839eded8fa0c5c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8923985362052917, 'eval_runtime': 206.3531, 'eval_samples_per_second': 0.654, 'eval_steps_per_second': 0.082, 'epoch': 2.29}\n",
      "{'loss': 0.5092, 'grad_norm': 0.49138087034225464, 'learning_rate': 2.2314572495745746e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4653, 'grad_norm': 0.40004271268844604, 'learning_rate': 1.8129115557213262e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aed9a0fdc424b4f863257179255bc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8872116208076477, 'eval_runtime': 207.9782, 'eval_samples_per_second': 0.649, 'eval_steps_per_second': 0.082, 'epoch': 2.41}\n",
      "{'loss': 0.536, 'grad_norm': 0.441256046295166, 'learning_rate': 1.433900477131882e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4915, 'grad_norm': 0.43019312620162964, 'learning_rate': 1.0962542196571634e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d06b4a46474fbda25b62096cc8f79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8817506432533264, 'eval_runtime': 210.2621, 'eval_samples_per_second': 0.642, 'eval_steps_per_second': 0.081, 'epoch': 2.54}\n",
      "{'loss': 0.5002, 'grad_norm': 0.47106724977493286, 'learning_rate': 8.016032426448817e-06, 'epoch': 2.6}\n",
      "{'loss': 0.4908, 'grad_norm': 0.5028514266014099, 'learning_rate': 5.5137038561761115e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7246ff0a514765ac2fd41a494ec2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8831735253334045, 'eval_runtime': 210.3494, 'eval_samples_per_second': 0.642, 'eval_steps_per_second': 0.081, 'epoch': 2.67}\n",
      "{'loss': 0.5375, 'grad_norm': 0.49816927313804626, 'learning_rate': 3.467639975257997e-06, 'epoch': 2.73}\n",
      "{'loss': 0.4582, 'grad_norm': 0.45445743203163147, 'learning_rate': 1.88772101753929e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b417d9047948a49a177d08617bd293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8836924433708191, 'eval_runtime': 208.0274, 'eval_samples_per_second': 0.649, 'eval_steps_per_second': 0.082, 'epoch': 2.79}\n",
      "{'loss': 0.501, 'grad_norm': 0.44629451632499695, 'learning_rate': 7.815762505632096e-07, 'epoch': 2.86}\n",
      "{'loss': 0.5477, 'grad_norm': 0.4729022979736328, 'learning_rate': 1.545471346164007e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48d658379d043cda2154e5ceb577163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8837672472000122, 'eval_runtime': 206.7775, 'eval_samples_per_second': 0.653, 'eval_steps_per_second': 0.082, 'epoch': 2.92}\n",
      "{'train_runtime': 7362.4668, 'train_samples_per_second': 0.257, 'train_steps_per_second': 0.032, 'train_loss': 0.7756713343481733, 'epoch': 2.97}\n",
      "Model saved to: ../models/trained-llama3-sentences_66agree\n",
      "\n",
      "Evaluating fine-tuned model on 135 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/135 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|          | 1/135 [00:00<00:34,  3.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|▏         | 2/135 [00:00<00:29,  4.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   2%|▏         | 3/135 [00:00<00:26,  4.90it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   3%|▎         | 4/135 [00:00<00:25,  5.11it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▎         | 5/135 [00:01<00:25,  5.10it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▍         | 6/135 [00:01<00:24,  5.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 7/135 [00:01<00:23,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   6%|▌         | 8/135 [00:01<00:23,  5.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 9/135 [00:01<00:23,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 10/135 [00:01<00:23,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   8%|▊         | 11/135 [00:02<00:22,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   9%|▉         | 12/135 [00:02<00:22,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|▉         | 13/135 [00:02<00:22,  5.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 14/135 [00:02<00:22,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  11%|█         | 15/135 [00:02<00:22,  5.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  12%|█▏        | 16/135 [00:03<00:22,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 17/135 [00:03<00:22,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 18/135 [00:03<00:21,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  14%|█▍        | 19/135 [00:03<00:21,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▍        | 20/135 [00:03<00:21,  5.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▌        | 21/135 [00:03<00:21,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▋        | 22/135 [00:04<00:21,  5.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  17%|█▋        | 23/135 [00:04<00:22,  4.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  18%|█▊        | 24/135 [00:04<00:22,  4.88it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▊        | 25/135 [00:04<00:22,  4.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▉        | 26/135 [00:04<00:21,  5.04it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 27/135 [00:05<00:20,  5.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██        | 28/135 [00:05<00:20,  5.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██▏       | 29/135 [00:05<00:19,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  22%|██▏       | 30/135 [00:05<00:19,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  23%|██▎       | 31/135 [00:05<00:19,  5.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▎       | 32/135 [00:06<00:18,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▍       | 33/135 [00:06<00:18,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 34/135 [00:06<00:18,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  26%|██▌       | 35/135 [00:06<00:18,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 36/135 [00:06<00:18,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 37/135 [00:07<00:18,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  28%|██▊       | 38/135 [00:07<00:17,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  29%|██▉       | 39/135 [00:07<00:17,  5.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|██▉       | 40/135 [00:07<00:17,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 41/135 [00:07<00:17,  5.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  31%|███       | 42/135 [00:07<00:18,  5.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  32%|███▏      | 43/135 [00:08<00:17,  5.11it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 44/135 [00:08<00:18,  5.03it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 45/135 [00:08<00:18,  4.95it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  34%|███▍      | 46/135 [00:08<00:17,  5.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▍      | 47/135 [00:08<00:16,  5.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▌      | 48/135 [00:09<00:16,  5.17it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▋      | 49/135 [00:09<00:16,  5.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  37%|███▋      | 50/135 [00:09<00:16,  5.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  38%|███▊      | 51/135 [00:09<00:16,  5.17it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▊      | 52/135 [00:09<00:15,  5.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▉      | 53/135 [00:10<00:15,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 54/135 [00:10<00:15,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████      | 55/135 [00:10<00:14,  5.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████▏     | 56/135 [00:10<00:14,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  42%|████▏     | 57/135 [00:10<00:14,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  43%|████▎     | 58/135 [00:11<00:14,  5.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▎     | 59/135 [00:11<00:14,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▍     | 60/135 [00:11<00:13,  5.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 61/135 [00:11<00:13,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  46%|████▌     | 62/135 [00:11<00:13,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 63/135 [00:11<00:13,  5.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 64/135 [00:12<00:13,  5.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  48%|████▊     | 65/135 [00:12<00:13,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  49%|████▉     | 66/135 [00:12<00:13,  5.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|████▉     | 67/135 [00:12<00:13,  5.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 68/135 [00:12<00:12,  5.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  51%|█████     | 69/135 [00:13<00:12,  5.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  52%|█████▏    | 70/135 [00:13<00:12,  5.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 71/135 [00:13<00:12,  5.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 72/135 [00:13<00:11,  5.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  54%|█████▍    | 73/135 [00:13<00:11,  5.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▍    | 74/135 [00:14<00:11,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▌    | 75/135 [00:14<00:11,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▋    | 76/135 [00:14<00:10,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  57%|█████▋    | 77/135 [00:14<00:10,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  58%|█████▊    | 78/135 [00:14<00:10,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▊    | 79/135 [00:14<00:10,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▉    | 80/135 [00:15<00:10,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 81/135 [00:15<00:10,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████    | 82/135 [00:15<00:09,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████▏   | 83/135 [00:15<00:09,  5.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  62%|██████▏   | 84/135 [00:15<00:09,  5.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  63%|██████▎   | 85/135 [00:16<00:09,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▎   | 86/135 [00:16<00:09,  5.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▍   | 87/135 [00:16<00:08,  5.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 88/135 [00:16<00:08,  5.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  66%|██████▌   | 89/135 [00:16<00:08,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 90/135 [00:17<00:08,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 91/135 [00:17<00:08,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  68%|██████▊   | 92/135 [00:17<00:07,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  69%|██████▉   | 93/135 [00:17<00:07,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|██████▉   | 94/135 [00:17<00:07,  5.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 95/135 [00:17<00:07,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  71%|███████   | 96/135 [00:18<00:07,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  72%|███████▏  | 97/135 [00:18<00:07,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 98/135 [00:18<00:06,  5.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 99/135 [00:18<00:06,  5.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  74%|███████▍  | 100/135 [00:18<00:06,  5.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▍  | 101/135 [00:19<00:06,  5.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▌  | 102/135 [00:19<00:06,  5.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▋  | 103/135 [00:19<00:05,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  77%|███████▋  | 104/135 [00:19<00:05,  5.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  78%|███████▊  | 105/135 [00:19<00:05,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▊  | 106/135 [00:20<00:05,  5.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▉  | 107/135 [00:20<00:05,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 108/135 [00:20<00:04,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████  | 109/135 [00:20<00:04,  5.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████▏ | 110/135 [00:20<00:04,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  82%|████████▏ | 111/135 [00:20<00:04,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  83%|████████▎ | 112/135 [00:21<00:04,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▎ | 113/135 [00:21<00:04,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▍ | 114/135 [00:21<00:03,  5.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 115/135 [00:21<00:03,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  86%|████████▌ | 116/135 [00:21<00:03,  5.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 117/135 [00:22<00:03,  5.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 118/135 [00:22<00:03,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  88%|████████▊ | 119/135 [00:22<00:02,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  89%|████████▉ | 120/135 [00:22<00:02,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|████████▉ | 121/135 [00:22<00:02,  5.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 122/135 [00:22<00:02,  5.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  91%|█████████ | 123/135 [00:23<00:02,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  92%|█████████▏| 124/135 [00:23<00:01,  5.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 125/135 [00:23<00:01,  5.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 126/135 [00:23<00:01,  5.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  94%|█████████▍| 127/135 [00:23<00:01,  5.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▍| 128/135 [00:24<00:01,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▌| 129/135 [00:24<00:01,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▋| 130/135 [00:24<00:00,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  97%|█████████▋| 131/135 [00:24<00:00,  5.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  98%|█████████▊| 132/135 [00:24<00:00,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▊| 133/135 [00:24<00:00,  5.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▉| 134/135 [00:25<00:00,  5.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 135/135 [00:25<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS FOR SENTENCES_66AGREE\n",
      "============================================================\n",
      "Overall Accuracy: 0.911\n",
      "Negative Accuracy: 0.933\n",
      "Neutral Accuracy: 0.956\n",
      "Positive Accuracy: 0.844\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.93      0.97        45\n",
      "     Neutral       0.81      0.96      0.88        45\n",
      "    Positive       0.95      0.84      0.89        45\n",
      "\n",
      "    accuracy                           0.91       135\n",
      "   macro avg       0.92      0.91      0.91       135\n",
      "weighted avg       0.92      0.91      0.91       135\n",
      "\n",
      "\n",
      "Completed sentences_66agree\n",
      "Final accuracy: 0.911\n",
      "Improvement over baseline: 0.511\n",
      "Waiting 60 seconds...\n",
      "Clearing memory...\n",
      "GPU memory allocated: 0.02 GB\n",
      "GPU memory reserved: 1.04 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING SENTENCES_75AGREE (3/4)\n",
      "================================================================================\n",
      "\n",
      "Loading dataset: sentences_75agree...\n",
      "Dataset shape: (3453, 2)\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    2146\n",
      "2     887\n",
      "0     420\n",
      "Name: count, dtype: int64\n",
      "Split sizes per class: Train=210, Val=45, Test=45\n",
      "Final splits: Train=630, Val=135, Test=135\n",
      "\n",
      "Loading fresh model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631c7324c72c4059a19737868a23b09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing baseline performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 1/20 [00:01<00:35,  1.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 2/20 [00:02<00:15,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▌        | 3/20 [00:02<00:09,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 4/20 [00:02<00:06,  2.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 5/20 [00:02<00:04,  3.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 6/20 [00:02<00:03,  4.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▌      | 7/20 [00:02<00:02,  4.66it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 8/20 [00:02<00:02,  5.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 9/20 [00:03<00:01,  5.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 10/20 [00:03<00:01,  5.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▌    | 11/20 [00:03<00:01,  6.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 12/20 [00:03<00:01,  6.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 13/20 [00:03<00:01,  6.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 14/20 [00:03<00:00,  6.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▌  | 15/20 [00:03<00:00,  6.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 16/20 [00:04<00:00,  6.62it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 17/20 [00:04<00:00,  6.69it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 18/20 [00:04<00:00,  6.73it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▌| 19/20 [00:04<00:00,  6.77it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 20/20 [00:04<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on 20 samples: 0.450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c20e9783e02466e8369077679aca2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb03aec65534be4a489d3231e51a8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning for sentences_75agree...\n",
      "Training samples: 630\n",
      "Validation samples: 135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbf0aabdd0b447b946d70cd89bf9a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8732, 'grad_norm': 0.5393697619438171, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 1.8321, 'grad_norm': 0.8845611214637756, 'learning_rate': 0.00019996135574945544, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f3856c63344cd5a19c92906a841a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1338236331939697, 'eval_runtime': 219.7067, 'eval_samples_per_second': 0.614, 'eval_steps_per_second': 0.077, 'epoch': 0.13}\n",
      "{'loss': 1.091, 'grad_norm': 0.4055427610874176, 'learning_rate': 0.00019952695086820975, 'epoch': 0.19}\n",
      "{'loss': 1.0231, 'grad_norm': 0.7259054780006409, 'learning_rate': 0.00019861194048993863, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd45c98ed504affbee4065c313345d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9373804330825806, 'eval_runtime': 217.0687, 'eval_samples_per_second': 0.622, 'eval_steps_per_second': 0.078, 'epoch': 0.25}\n",
      "{'loss': 0.9119, 'grad_norm': 0.41434502601623535, 'learning_rate': 0.00019722074310645553, 'epoch': 0.32}\n",
      "{'loss': 0.9088, 'grad_norm': 0.3927311897277832, 'learning_rate': 0.00019536007666806556, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4dfddd35847b58dc29d723520b044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8770038485527039, 'eval_runtime': 216.4366, 'eval_samples_per_second': 0.624, 'eval_steps_per_second': 0.079, 'epoch': 0.38}\n",
      "{'loss': 0.8343, 'grad_norm': 0.32796576619148254, 'learning_rate': 0.00019303892614326836, 'epoch': 0.44}\n",
      "{'loss': 0.8306, 'grad_norm': 0.46996957063674927, 'learning_rate': 0.00019026850013126157, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6170742241e4ea9b6d729161bfdd992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8681343197822571, 'eval_runtime': 213.6059, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.08, 'epoch': 0.51}\n",
      "{'loss': 0.9763, 'grad_norm': 0.35498496890068054, 'learning_rate': 0.00018706217673675811, 'epoch': 0.57}\n",
      "{'loss': 0.899, 'grad_norm': 0.3547478914260864, 'learning_rate': 0.00018343543896848273, 'epoch': 0.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf932c22bed49dea11bed407ed4d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8263314962387085, 'eval_runtime': 222.3409, 'eval_samples_per_second': 0.607, 'eval_steps_per_second': 0.076, 'epoch': 0.63}\n",
      "{'loss': 0.7724, 'grad_norm': 0.26523557305336, 'learning_rate': 0.00017940579997330165, 'epoch': 0.7}\n",
      "{'loss': 0.8923, 'grad_norm': 0.3638657331466675, 'learning_rate': 0.00017499271846702213, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cada90a9d94c2b8fbf24de2956bde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8185635209083557, 'eval_runtime': 218.8139, 'eval_samples_per_second': 0.617, 'eval_steps_per_second': 0.078, 'epoch': 0.76}\n",
      "{'loss': 0.8327, 'grad_norm': 0.2931210994720459, 'learning_rate': 0.0001702175047702382, 'epoch': 0.83}\n",
      "{'loss': 0.8642, 'grad_norm': 0.24488259851932526, 'learning_rate': 0.00016510321790296525, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217ecbda738d4e8d9e48a05f287abc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8090540170669556, 'eval_runtime': 215.1266, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.079, 'epoch': 0.89}\n",
      "{'loss': 0.777, 'grad_norm': 0.24602742493152618, 'learning_rate': 0.00015967455423498387, 'epoch': 0.95}\n",
      "{'loss': 0.7644, 'grad_norm': 0.3128562271595001, 'learning_rate': 0.00015395772822958845, 'epoch': 1.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224f95710ef84263ae14b610ab802c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8089891076087952, 'eval_runtime': 216.6737, 'eval_samples_per_second': 0.623, 'eval_steps_per_second': 0.078, 'epoch': 1.02}\n",
      "{'loss': 0.7298, 'grad_norm': 0.23146384954452515, 'learning_rate': 0.00014798034585661695, 'epoch': 1.08}\n",
      "{'loss': 0.7617, 'grad_norm': 0.2591002285480499, 'learning_rate': 0.00014177127128603745, 'epoch': 1.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3560f599a04667b278e2dd024fbfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8133487105369568, 'eval_runtime': 221.1349, 'eval_samples_per_second': 0.61, 'eval_steps_per_second': 0.077, 'epoch': 1.14}\n",
      "{'loss': 0.7676, 'grad_norm': 0.28644463419914246, 'learning_rate': 0.00013536048750581494, 'epoch': 1.21}\n",
      "{'loss': 0.7226, 'grad_norm': 0.2920090854167938, 'learning_rate': 0.00012877895153711935, 'epoch': 1.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a36ec13b5e04fa29aefe0b10aa9d81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8161007165908813, 'eval_runtime': 216.4418, 'eval_samples_per_second': 0.624, 'eval_steps_per_second': 0.079, 'epoch': 1.27}\n",
      "{'loss': 0.6878, 'grad_norm': 0.2940739095211029, 'learning_rate': 0.0001220584449460274, 'epoch': 1.33}\n",
      "{'loss': 0.6505, 'grad_norm': 0.3880539536476135, 'learning_rate': 0.0001152314203735805, 'epoch': 1.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caf407f12684c6bb7987cb7ca0faa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8204386234283447, 'eval_runtime': 217.1192, 'eval_samples_per_second': 0.622, 'eval_steps_per_second': 0.078, 'epoch': 1.4}\n",
      "{'loss': 0.704, 'grad_norm': 0.30593281984329224, 'learning_rate': 0.00010833084482529048, 'epoch': 1.46}\n",
      "{'loss': 0.6447, 'grad_norm': 0.2970358431339264, 'learning_rate': 0.00010139004047683151, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa5a4924b2b4c4e94cc0e5ce3ade76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8163177967071533, 'eval_runtime': 214.8824, 'eval_samples_per_second': 0.628, 'eval_steps_per_second': 0.079, 'epoch': 1.52}\n",
      "{'loss': 0.6117, 'grad_norm': 0.3853822946548462, 'learning_rate': 9.444252376465171e-05, 'epoch': 1.59}\n",
      "{'loss': 0.6382, 'grad_norm': 0.35897135734558105, 'learning_rate': 8.752184353851916e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10826fe6209b44559f29c49df19156e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8250334858894348, 'eval_runtime': 216.757, 'eval_samples_per_second': 0.623, 'eval_steps_per_second': 0.078, 'epoch': 1.65}\n",
      "{'loss': 0.7086, 'grad_norm': 0.3674660921096802, 'learning_rate': 8.066141905754723e-05, 'epoch': 1.71}\n",
      "{'loss': 0.6403, 'grad_norm': 0.3801726698875427, 'learning_rate': 7.389437861200024e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd68dcc9c1541f1804fd354b3fff25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8075795769691467, 'eval_runtime': 221.2036, 'eval_samples_per_second': 0.61, 'eval_steps_per_second': 0.077, 'epoch': 1.78}\n",
      "{'loss': 0.7855, 'grad_norm': 0.3012968599796295, 'learning_rate': 6.725339955015777e-05, 'epoch': 1.84}\n",
      "{'loss': 0.6695, 'grad_norm': 0.31696459650993347, 'learning_rate': 6.0770550482731924e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5a64c81fdc48ac9534ddd790d62eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8047964572906494, 'eval_runtime': 216.5422, 'eval_samples_per_second': 0.623, 'eval_steps_per_second': 0.079, 'epoch': 1.9}\n",
      "{'loss': 0.7236, 'grad_norm': 0.3670088052749634, 'learning_rate': 5.447713642681612e-05, 'epoch': 1.97}\n",
      "{'loss': 0.5816, 'grad_norm': 0.2919301986694336, 'learning_rate': 4.840354763714991e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b95b382944312be6e17d42abc4dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8079885840415955, 'eval_runtime': 218.2693, 'eval_samples_per_second': 0.619, 'eval_steps_per_second': 0.078, 'epoch': 2.03}\n",
      "{'loss': 0.5719, 'grad_norm': 0.28881630301475525, 'learning_rate': 4.257911285467754e-05, 'epoch': 2.1}\n",
      "{'loss': 0.5246, 'grad_norm': 0.4111449420452118, 'learning_rate': 3.7031957681048604e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81537eaeb1aa49d2876c282eb028cc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8460132479667664, 'eval_runtime': 216.1742, 'eval_samples_per_second': 0.624, 'eval_steps_per_second': 0.079, 'epoch': 2.16}\n",
      "{'loss': 0.4804, 'grad_norm': 0.4492410123348236, 'learning_rate': 3.178886876295578e-05, 'epoch': 2.22}\n",
      "{'loss': 0.4856, 'grad_norm': 0.4509808123111725, 'learning_rate': 2.6875164442149147e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a35485882e6483c9517168f8fb24d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8626189827919006, 'eval_runtime': 216.0102, 'eval_samples_per_second': 0.625, 'eval_steps_per_second': 0.079, 'epoch': 2.29}\n",
      "{'loss': 0.567, 'grad_norm': 0.41515856981277466, 'learning_rate': 2.2314572495745746e-05, 'epoch': 2.35}\n",
      "{'loss': 0.5049, 'grad_norm': 0.36313995718955994, 'learning_rate': 1.8129115557213262e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249e2f47459b4da3a2c94dd6c5bc56df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8476336002349854, 'eval_runtime': 221.9463, 'eval_samples_per_second': 0.608, 'eval_steps_per_second': 0.077, 'epoch': 2.41}\n",
      "{'loss': 0.4733, 'grad_norm': 0.4357021450996399, 'learning_rate': 1.433900477131882e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4651, 'grad_norm': 0.4105619192123413, 'learning_rate': 1.0962542196571634e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d72ef1a50c489f9f7999577f487554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8472341895103455, 'eval_runtime': 222.0203, 'eval_samples_per_second': 0.608, 'eval_steps_per_second': 0.077, 'epoch': 2.54}\n",
      "{'loss': 0.4754, 'grad_norm': 0.39201226830482483, 'learning_rate': 8.016032426448817e-06, 'epoch': 2.6}\n",
      "{'loss': 0.4654, 'grad_norm': 0.3738134503364563, 'learning_rate': 5.5137038561761115e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e630d4c1c481495d849dc970093261be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8522493839263916, 'eval_runtime': 223.9052, 'eval_samples_per_second': 0.603, 'eval_steps_per_second': 0.076, 'epoch': 2.67}\n",
      "{'loss': 0.4862, 'grad_norm': 0.45464855432510376, 'learning_rate': 3.467639975257997e-06, 'epoch': 2.73}\n",
      "{'loss': 0.5554, 'grad_norm': 0.4134228825569153, 'learning_rate': 1.88772101753929e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240b5eeb43fb4348ac1f1545a2300a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8541361689567566, 'eval_runtime': 219.8418, 'eval_samples_per_second': 0.614, 'eval_steps_per_second': 0.077, 'epoch': 2.79}\n",
      "{'loss': 0.4825, 'grad_norm': 0.4669239819049835, 'learning_rate': 7.815762505632096e-07, 'epoch': 2.86}\n",
      "{'loss': 0.5167, 'grad_norm': 0.5111904740333557, 'learning_rate': 1.545471346164007e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a61297f93646f8a29ec809231765cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8541510701179504, 'eval_runtime': 219.019, 'eval_samples_per_second': 0.616, 'eval_steps_per_second': 0.078, 'epoch': 2.92}\n",
      "{'train_runtime': 8013.7805, 'train_samples_per_second': 0.236, 'train_steps_per_second': 0.029, 'train_loss': 0.7601264531795795, 'epoch': 2.97}\n",
      "Model saved to: ../models/trained-llama3-sentences_75agree\n",
      "\n",
      "Evaluating fine-tuned model on 135 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/135 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|          | 1/135 [00:01<03:30,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|▏         | 2/135 [00:03<03:39,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   2%|▏         | 3/135 [00:04<02:56,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   3%|▎         | 4/135 [00:05<02:53,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▎         | 5/135 [00:06<02:50,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▍         | 6/135 [00:08<02:51,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 7/135 [00:10<03:19,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   6%|▌         | 8/135 [00:11<02:53,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 9/135 [00:12<02:40,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 10/135 [00:13<02:38,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   8%|▊         | 11/135 [00:15<02:53,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   9%|▉         | 12/135 [00:16<02:42,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|▉         | 13/135 [00:17<02:28,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 14/135 [00:19<02:43,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  11%|█         | 15/135 [00:20<02:40,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  12%|█▏        | 16/135 [00:21<02:50,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 17/135 [00:23<02:36,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 18/135 [00:23<02:21,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  14%|█▍        | 19/135 [00:25<02:23,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▍        | 20/135 [00:26<02:22,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▌        | 21/135 [00:28<02:39,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▋        | 22/135 [00:29<02:32,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  17%|█▋        | 23/135 [00:30<02:22,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  18%|█▊        | 24/135 [00:31<02:19,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▊        | 25/135 [00:33<02:20,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▉        | 26/135 [00:34<02:20,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 27/135 [00:35<02:11,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██        | 28/135 [00:36<02:10,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██▏       | 29/135 [00:38<02:11,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  22%|██▏       | 30/135 [00:39<02:12,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  23%|██▎       | 31/135 [00:40<02:01,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▎       | 32/135 [00:41<01:54,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▍       | 33/135 [00:42<01:52,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 34/135 [00:43<01:47,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  26%|██▌       | 35/135 [00:44<01:53,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 36/135 [00:45<01:48,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 37/135 [00:46<01:52,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  28%|██▊       | 38/135 [00:48<02:05,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  29%|██▉       | 39/135 [00:49<01:57,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|██▉       | 40/135 [00:50<01:51,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 41/135 [00:51<01:44,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  31%|███       | 42/135 [00:52<01:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  32%|███▏      | 43/135 [00:53<01:40,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 44/135 [00:55<01:44,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 45/135 [00:56<01:39,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  34%|███▍      | 46/135 [00:57<01:37,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▍      | 47/135 [00:58<01:36,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▌      | 48/135 [01:00<01:55,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▋      | 49/135 [01:01<01:53,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  37%|███▋      | 50/135 [01:02<01:43,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  38%|███▊      | 51/135 [01:03<01:36,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▊      | 52/135 [01:04<01:37,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▉      | 53/135 [01:05<01:39,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 54/135 [01:06<01:32,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████      | 55/135 [01:08<01:33,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████▏     | 56/135 [01:09<01:27,  1.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  42%|████▏     | 57/135 [01:10<01:42,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  43%|████▎     | 58/135 [01:12<01:48,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▎     | 59/135 [01:13<01:37,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▍     | 60/135 [01:15<01:45,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 61/135 [01:16<01:33,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  46%|████▌     | 62/135 [01:17<01:41,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 63/135 [01:19<01:45,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 64/135 [01:20<01:35,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  48%|████▊     | 65/135 [01:22<01:44,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  49%|████▉     | 66/135 [01:23<01:38,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|████▉     | 67/135 [01:25<01:44,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 68/135 [01:26<01:33,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  51%|█████     | 69/135 [01:28<01:42,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  52%|█████▏    | 70/135 [01:29<01:35,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 71/135 [01:30<01:24,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 72/135 [01:31<01:16,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  54%|█████▍    | 73/135 [01:32<01:09,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▍    | 74/135 [01:34<01:19,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▌    | 75/135 [01:37<01:55,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▋    | 76/135 [01:39<01:43,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  57%|█████▋    | 77/135 [01:40<01:43,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  58%|█████▊    | 78/135 [01:41<01:29,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▊    | 79/135 [01:42<01:16,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▉    | 80/135 [01:44<01:23,  1.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 81/135 [01:46<01:19,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████    | 82/135 [01:47<01:13,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████▏   | 83/135 [01:49<01:17,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  62%|██████▏   | 84/135 [01:50<01:13,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  63%|██████▎   | 85/135 [01:51<01:09,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▎   | 86/135 [01:53<01:13,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▍   | 87/135 [01:54<01:08,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 88/135 [01:55<01:03,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  66%|██████▌   | 89/135 [01:57<01:01,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 90/135 [01:58<01:04,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 91/135 [01:59<00:56,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  68%|██████▊   | 92/135 [02:01<00:59,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  69%|██████▉   | 93/135 [02:02<01:01,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|██████▉   | 94/135 [02:04<01:04,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 95/135 [02:05<00:58,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  71%|███████   | 96/135 [02:07<00:52,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  72%|███████▏  | 97/135 [02:08<00:56,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 98/135 [02:10<00:56,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 99/135 [02:11<00:49,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  74%|███████▍  | 100/135 [02:12<00:47,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▍  | 101/135 [02:14<00:45,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▌  | 102/135 [02:15<00:44,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▋  | 103/135 [02:16<00:39,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  77%|███████▋  | 104/135 [02:17<00:38,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  78%|███████▊  | 105/135 [02:18<00:34,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▊  | 106/135 [02:19<00:34,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▉  | 107/135 [02:20<00:31,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 108/135 [02:21<00:29,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████  | 109/135 [02:22<00:27,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████▏ | 110/135 [02:23<00:25,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  82%|████████▏ | 111/135 [02:25<00:29,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  83%|████████▎ | 112/135 [02:26<00:25,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▎ | 113/135 [02:28<00:28,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▍ | 114/135 [02:29<00:27,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 115/135 [02:30<00:25,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  86%|████████▌ | 116/135 [02:31<00:22,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 117/135 [02:32<00:22,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 118/135 [02:34<00:23,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  88%|████████▊ | 119/135 [02:35<00:21,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  89%|████████▉ | 120/135 [02:36<00:18,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|████████▉ | 121/135 [02:38<00:17,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 122/135 [02:39<00:15,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  91%|█████████ | 123/135 [02:41<00:16,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  92%|█████████▏| 124/135 [02:42<00:13,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 125/135 [02:43<00:12,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 126/135 [02:45<00:12,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  94%|█████████▍| 127/135 [02:46<00:10,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▍| 128/135 [02:47<00:09,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▌| 129/135 [02:49<00:08,  1.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▋| 130/135 [02:50<00:06,  1.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  97%|█████████▋| 131/135 [02:52<00:05,  1.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  98%|█████████▊| 132/135 [02:53<00:04,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▊| 133/135 [02:55<00:02,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▉| 134/135 [02:56<00:01,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 135/135 [02:57<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS FOR SENTENCES_75AGREE\n",
      "============================================================\n",
      "Overall Accuracy: 0.963\n",
      "Negative Accuracy: 0.978\n",
      "Neutral Accuracy: 0.933\n",
      "Positive Accuracy: 0.978\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.98      0.97        45\n",
      "     Neutral       0.95      0.93      0.94        45\n",
      "    Positive       0.98      0.98      0.98        45\n",
      "\n",
      "    accuracy                           0.96       135\n",
      "   macro avg       0.96      0.96      0.96       135\n",
      "weighted avg       0.96      0.96      0.96       135\n",
      "\n",
      "\n",
      "Completed sentences_75agree\n",
      "Final accuracy: 0.963\n",
      "Improvement over baseline: 0.513\n",
      "Waiting 60 seconds...\n",
      "Clearing memory...\n",
      "GPU memory allocated: 0.02 GB\n",
      "GPU memory reserved: 1.04 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING SENTENCES_ALLAGREE (4/4)\n",
      "================================================================================\n",
      "\n",
      "Loading dataset: sentences_allagree...\n",
      "Dataset shape: (2264, 2)\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "1    1391\n",
      "2     570\n",
      "0     303\n",
      "Name: count, dtype: int64\n",
      "Split sizes per class: Train=210, Val=45, Test=45\n",
      "Final splits: Train=630, Val=135, Test=135\n",
      "\n",
      "Loading fresh model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f7cb5c863c489fb8598fb43261e114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing baseline performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 1/20 [00:02<00:44,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 2/20 [00:02<00:19,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▌        | 3/20 [00:02<00:10,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 4/20 [00:02<00:07,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 5/20 [00:02<00:05,  2.91it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 6/20 [00:03<00:03,  3.59it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▌      | 7/20 [00:03<00:03,  4.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 8/20 [00:03<00:02,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 9/20 [00:03<00:02,  5.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 10/20 [00:03<00:01,  5.58it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▌    | 11/20 [00:03<00:01,  5.81it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 12/20 [00:04<00:01,  6.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 13/20 [00:04<00:01,  6.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 14/20 [00:04<00:00,  6.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▌  | 15/20 [00:04<00:00,  6.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 16/20 [00:04<00:00,  6.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 17/20 [00:04<00:00,  6.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 18/20 [00:04<00:00,  6.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▌| 19/20 [00:05<00:00,  6.57it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 20/20 [00:05<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on 20 samples: 0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb3f15bf67b498b950e9b40b9884ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a414625201d840e39b28c8512e76f224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning for sentences_allagree...\n",
      "Training samples: 630\n",
      "Validation samples: 135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6255b4716c3842878450501055227680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.879, 'grad_norm': 0.5210840702056885, 'learning_rate': 0.000125, 'epoch': 0.06}\n",
      "{'loss': 1.7489, 'grad_norm': 0.9715730547904968, 'learning_rate': 0.00019996135574945544, 'epoch': 0.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e58eb1be9441679f51a11dc97d50c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0943827629089355, 'eval_runtime': 174.5634, 'eval_samples_per_second': 0.773, 'eval_steps_per_second': 0.097, 'epoch': 0.13}\n",
      "{'loss': 0.9792, 'grad_norm': 0.5262844562530518, 'learning_rate': 0.00019952695086820975, 'epoch': 0.19}\n",
      "{'loss': 0.9663, 'grad_norm': 0.5234163403511047, 'learning_rate': 0.00019861194048993863, 'epoch': 0.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0321fdb210046dabb4cf1d81dfc149d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9017727375030518, 'eval_runtime': 155.7085, 'eval_samples_per_second': 0.867, 'eval_steps_per_second': 0.109, 'epoch': 0.25}\n",
      "{'loss': 0.918, 'grad_norm': 0.3805803954601288, 'learning_rate': 0.00019722074310645553, 'epoch': 0.32}\n",
      "{'loss': 0.8599, 'grad_norm': 0.3570477366447449, 'learning_rate': 0.00019536007666806556, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fc752388a44249bf4ad1af44a62519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8367169499397278, 'eval_runtime': 157.6639, 'eval_samples_per_second': 0.856, 'eval_steps_per_second': 0.108, 'epoch': 0.38}\n",
      "{'loss': 0.8566, 'grad_norm': 0.2690417766571045, 'learning_rate': 0.00019303892614326836, 'epoch': 0.44}\n",
      "{'loss': 0.7909, 'grad_norm': 0.26692622900009155, 'learning_rate': 0.00019026850013126157, 'epoch': 0.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f8c42e801d43b79f2e4fbad528fa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8157224059104919, 'eval_runtime': 281.9759, 'eval_samples_per_second': 0.479, 'eval_steps_per_second': 0.06, 'epoch': 0.51}\n",
      "{'loss': 0.7339, 'grad_norm': 0.32899463176727295, 'learning_rate': 0.00018706217673675811, 'epoch': 0.57}\n",
      "{'loss': 0.7448, 'grad_norm': 0.23386503756046295, 'learning_rate': 0.00018343543896848273, 'epoch': 0.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555e83df8b8b49a7bf5c9399bb692a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8091524839401245, 'eval_runtime': 333.6511, 'eval_samples_per_second': 0.405, 'eval_steps_per_second': 0.051, 'epoch': 0.63}\n",
      "{'loss': 0.8291, 'grad_norm': 0.2396279275417328, 'learning_rate': 0.00017940579997330165, 'epoch': 0.7}\n",
      "{'loss': 0.8183, 'grad_norm': 0.28816738724708557, 'learning_rate': 0.00017499271846702213, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4d77626dac4a8bb4e2a6310f1aebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7917585372924805, 'eval_runtime': 330.8633, 'eval_samples_per_second': 0.408, 'eval_steps_per_second': 0.051, 'epoch': 0.76}\n",
      "{'loss': 0.7838, 'grad_norm': 0.3204214572906494, 'learning_rate': 0.0001702175047702382, 'epoch': 0.83}\n",
      "{'loss': 0.7703, 'grad_norm': 0.3157523572444916, 'learning_rate': 0.00016510321790296525, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6c2a739f564780af161aece4b0e513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7821682691574097, 'eval_runtime': 329.8693, 'eval_samples_per_second': 0.409, 'eval_steps_per_second': 0.052, 'epoch': 0.89}\n",
      "{'loss': 0.8344, 'grad_norm': 0.2941927909851074, 'learning_rate': 0.00015967455423498387, 'epoch': 0.95}\n",
      "{'loss': 0.7386, 'grad_norm': 0.19458281993865967, 'learning_rate': 0.00015395772822958845, 'epoch': 1.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab27151d8fe44f7851a88ecda734f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7822928428649902, 'eval_runtime': 331.1346, 'eval_samples_per_second': 0.408, 'eval_steps_per_second': 0.051, 'epoch': 1.02}\n",
      "{'loss': 0.6381, 'grad_norm': 0.2493957132101059, 'learning_rate': 0.00014798034585661695, 'epoch': 1.08}\n",
      "{'loss': 0.6227, 'grad_norm': 0.3655543923377991, 'learning_rate': 0.00014177127128603745, 'epoch': 1.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf0d2a961ad483fb1b820cd460973d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7965009808540344, 'eval_runtime': 320.855, 'eval_samples_per_second': 0.421, 'eval_steps_per_second': 0.053, 'epoch': 1.14}\n",
      "{'loss': 0.621, 'grad_norm': 0.26448148488998413, 'learning_rate': 0.00013536048750581494, 'epoch': 1.21}\n",
      "{'loss': 0.6734, 'grad_norm': 0.2443794459104538, 'learning_rate': 0.00012877895153711935, 'epoch': 1.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e389ae328d1d4c20a5f16e9e2cc4f5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7839676141738892, 'eval_runtime': 335.1248, 'eval_samples_per_second': 0.403, 'eval_steps_per_second': 0.051, 'epoch': 1.27}\n",
      "{'loss': 0.6927, 'grad_norm': 0.5456637144088745, 'learning_rate': 0.0001220584449460274, 'epoch': 1.33}\n",
      "{'loss': 0.6791, 'grad_norm': 0.32138553261756897, 'learning_rate': 0.0001152314203735805, 'epoch': 1.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee10938fcf2b4ec385f637e1f56a8e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7837921380996704, 'eval_runtime': 343.3427, 'eval_samples_per_second': 0.393, 'eval_steps_per_second': 0.05, 'epoch': 1.4}\n",
      "{'loss': 0.6793, 'grad_norm': 0.2975497841835022, 'learning_rate': 0.00010833084482529048, 'epoch': 1.46}\n",
      "{'loss': 0.5684, 'grad_norm': 0.30668407678604126, 'learning_rate': 0.00010139004047683151, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eee2563a1f488287c7681f181739d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7946357727050781, 'eval_runtime': 356.7692, 'eval_samples_per_second': 0.378, 'eval_steps_per_second': 0.048, 'epoch': 1.52}\n",
      "{'loss': 0.6218, 'grad_norm': 0.3050818145275116, 'learning_rate': 9.444252376465171e-05, 'epoch': 1.59}\n",
      "{'loss': 0.6844, 'grad_norm': 0.2932189106941223, 'learning_rate': 8.752184353851916e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873a983b799546e98eb589abaad5b2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7911360859870911, 'eval_runtime': 346.2204, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 1.65}\n",
      "{'loss': 0.5891, 'grad_norm': 0.29990899562835693, 'learning_rate': 8.066141905754723e-05, 'epoch': 1.71}\n",
      "{'loss': 0.6925, 'grad_norm': 0.5221179127693176, 'learning_rate': 7.389437861200024e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a716899105724b74869bd7b9199d74fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7838712334632874, 'eval_runtime': 346.5302, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 1.78}\n",
      "{'loss': 0.6611, 'grad_norm': 0.26235347986221313, 'learning_rate': 6.725339955015777e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5832, 'grad_norm': 0.3248666226863861, 'learning_rate': 6.0770550482731924e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb338c6e99f47958118d9e38961e22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7827178835868835, 'eval_runtime': 345.1218, 'eval_samples_per_second': 0.391, 'eval_steps_per_second': 0.049, 'epoch': 1.9}\n",
      "{'loss': 0.5858, 'grad_norm': 0.37818285822868347, 'learning_rate': 5.447713642681612e-05, 'epoch': 1.97}\n",
      "{'loss': 0.5354, 'grad_norm': 0.27961787581443787, 'learning_rate': 4.840354763714991e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0636adcdfb734948b45ec42e924956df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7843722701072693, 'eval_runtime': 344.3247, 'eval_samples_per_second': 0.392, 'eval_steps_per_second': 0.049, 'epoch': 2.03}\n",
      "{'loss': 0.5218, 'grad_norm': 0.292674720287323, 'learning_rate': 4.257911285467754e-05, 'epoch': 2.1}\n",
      "{'loss': 0.4874, 'grad_norm': 0.35253503918647766, 'learning_rate': 3.7031957681048604e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33ae32e3a41463b82f5de2c8e0392b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8154512643814087, 'eval_runtime': 346.0057, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 2.16}\n",
      "{'loss': 0.4377, 'grad_norm': 0.43229809403419495, 'learning_rate': 3.178886876295578e-05, 'epoch': 2.22}\n",
      "{'loss': 0.4937, 'grad_norm': 0.42884498834609985, 'learning_rate': 2.6875164442149147e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12887430e0694b1c89111782c220216e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8285250663757324, 'eval_runtime': 345.8385, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 2.29}\n",
      "{'loss': 0.4929, 'grad_norm': 0.4064130187034607, 'learning_rate': 2.2314572495745746e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4901, 'grad_norm': 0.47014114260673523, 'learning_rate': 1.8129115557213262e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef63565b8ade45d98e53b39cacb4514c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8230857849121094, 'eval_runtime': 346.2235, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 2.41}\n",
      "{'loss': 0.497, 'grad_norm': 0.4197785556316376, 'learning_rate': 1.433900477131882e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4396, 'grad_norm': 0.44456347823143005, 'learning_rate': 1.0962542196571634e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9109164344a046ec93ac64dc00dd8fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8221232891082764, 'eval_runtime': 344.0138, 'eval_samples_per_second': 0.392, 'eval_steps_per_second': 0.049, 'epoch': 2.54}\n",
      "{'loss': 0.4602, 'grad_norm': 0.3751881420612335, 'learning_rate': 8.016032426448817e-06, 'epoch': 2.6}\n",
      "{'loss': 0.4307, 'grad_norm': 0.45388710498809814, 'learning_rate': 5.5137038561761115e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce16f46566d84ef288aaafcf7e213e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8235680460929871, 'eval_runtime': 346.0437, 'eval_samples_per_second': 0.39, 'eval_steps_per_second': 0.049, 'epoch': 2.67}\n",
      "{'loss': 0.4622, 'grad_norm': 0.4631974995136261, 'learning_rate': 3.467639975257997e-06, 'epoch': 2.73}\n",
      "{'loss': 0.4497, 'grad_norm': 0.4747866690158844, 'learning_rate': 1.88772101753929e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7836141c4c40fa9e0261c28dfbc4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8236562609672546, 'eval_runtime': 345.4396, 'eval_samples_per_second': 0.391, 'eval_steps_per_second': 0.049, 'epoch': 2.79}\n",
      "{'loss': 0.4795, 'grad_norm': 0.44055768847465515, 'learning_rate': 7.815762505632096e-07, 'epoch': 2.86}\n",
      "{'loss': 0.4512, 'grad_norm': 0.41676968336105347, 'learning_rate': 1.545471346164007e-07, 'epoch': 2.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ace0dde4cd4f4c9c9c704888ea0017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8235984444618225, 'eval_runtime': 344.2162, 'eval_samples_per_second': 0.392, 'eval_steps_per_second': 0.049, 'epoch': 2.92}\n",
      "{'train_runtime': 17526.6598, 'train_samples_per_second': 0.108, 'train_steps_per_second': 0.013, 'train_loss': 0.7132101629534339, 'epoch': 2.97}\n",
      "Model saved to: ../models/trained-llama3-sentences_allagree\n",
      "\n",
      "Evaluating fine-tuned model on 135 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/135 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|          | 1/135 [00:04<09:16,  4.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   1%|▏         | 2/135 [00:08<09:00,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   2%|▏         | 3/135 [00:12<08:50,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   3%|▎         | 4/135 [00:15<08:23,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▎         | 5/135 [00:19<08:12,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   4%|▍         | 6/135 [00:22<07:58,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   5%|▌         | 7/135 [00:26<07:47,  3.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   6%|▌         | 8/135 [00:30<07:39,  3.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 9/135 [00:34<07:52,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   7%|▋         | 10/135 [00:37<07:55,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   8%|▊         | 11/135 [00:42<08:15,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:   9%|▉         | 12/135 [00:46<08:11,  3.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|▉         | 13/135 [00:49<07:51,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  10%|█         | 14/135 [00:53<07:39,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  11%|█         | 15/135 [00:57<07:42,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  12%|█▏        | 16/135 [01:01<07:27,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 17/135 [01:05<07:44,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  13%|█▎        | 18/135 [01:09<07:42,  3.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  14%|█▍        | 19/135 [01:13<07:53,  4.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  15%|█▍        | 20/135 [01:17<07:47,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▌        | 21/135 [01:21<07:38,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  16%|█▋        | 22/135 [01:25<07:32,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  17%|█▋        | 23/135 [01:29<07:19,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  18%|█▊        | 24/135 [01:33<07:06,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▊        | 25/135 [01:36<06:53,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  19%|█▉        | 26/135 [01:40<06:52,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  20%|██        | 27/135 [01:44<06:39,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██        | 28/135 [01:48<06:53,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  21%|██▏       | 29/135 [01:51<06:39,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  22%|██▏       | 30/135 [01:55<06:38,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  23%|██▎       | 31/135 [01:59<06:38,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▎       | 32/135 [02:03<06:36,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  24%|██▍       | 33/135 [02:07<06:33,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  25%|██▌       | 34/135 [02:10<06:20,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  26%|██▌       | 35/135 [02:14<06:21,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 36/135 [02:19<06:34,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  27%|██▋       | 37/135 [02:23<06:25,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  28%|██▊       | 38/135 [02:26<06:10,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  29%|██▉       | 39/135 [02:30<06:09,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|██▉       | 40/135 [02:34<06:11,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  30%|███       | 41/135 [02:38<06:06,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  31%|███       | 42/135 [02:42<06:11,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  32%|███▏      | 43/135 [02:46<05:54,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 44/135 [02:50<05:51,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  33%|███▎      | 45/135 [02:54<06:01,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  34%|███▍      | 46/135 [02:57<05:42,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  35%|███▍      | 47/135 [03:01<05:32,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▌      | 48/135 [03:05<05:27,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  36%|███▋      | 49/135 [03:08<05:18,  3.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  37%|███▋      | 50/135 [03:12<05:24,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  38%|███▊      | 51/135 [03:17<05:26,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▊      | 52/135 [03:21<05:33,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  39%|███▉      | 53/135 [03:25<05:36,  4.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  40%|████      | 54/135 [03:29<05:29,  4.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████      | 55/135 [03:33<05:32,  4.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  41%|████▏     | 56/135 [03:37<05:14,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  42%|████▏     | 57/135 [03:41<05:09,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  43%|████▎     | 58/135 [03:45<05:06,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▎     | 59/135 [03:49<04:55,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  44%|████▍     | 60/135 [03:52<04:45,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  45%|████▌     | 61/135 [03:56<04:37,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  46%|████▌     | 62/135 [04:00<04:39,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 63/135 [04:04<04:38,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  47%|████▋     | 64/135 [04:08<04:46,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  48%|████▊     | 65/135 [04:12<04:32,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  49%|████▉     | 66/135 [04:15<04:21,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|████▉     | 67/135 [04:19<04:22,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  50%|█████     | 68/135 [04:23<04:13,  3.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  51%|█████     | 69/135 [04:27<04:11,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  52%|█████▏    | 70/135 [04:31<04:17,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 71/135 [04:35<04:08,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  53%|█████▎    | 72/135 [04:39<04:04,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  54%|█████▍    | 73/135 [04:43<04:03,  3.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  55%|█████▍    | 74/135 [04:47<04:00,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▌    | 75/135 [04:50<03:50,  3.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  56%|█████▋    | 76/135 [04:54<03:47,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  57%|█████▋    | 77/135 [04:58<03:45,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  58%|█████▊    | 78/135 [05:02<03:36,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▊    | 79/135 [05:05<03:28,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  59%|█████▉    | 80/135 [05:09<03:28,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  60%|██████    | 81/135 [05:13<03:21,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████    | 82/135 [05:17<03:22,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  61%|██████▏   | 83/135 [05:21<03:20,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  62%|██████▏   | 84/135 [05:25<03:17,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  63%|██████▎   | 85/135 [05:29<03:14,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▎   | 86/135 [05:33<03:12,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  64%|██████▍   | 87/135 [05:37<03:17,  4.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  65%|██████▌   | 88/135 [05:41<03:10,  4.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  66%|██████▌   | 89/135 [05:45<03:01,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 90/135 [05:48<02:52,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  67%|██████▋   | 91/135 [05:52<02:49,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  68%|██████▊   | 92/135 [05:56<02:47,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  69%|██████▉   | 93/135 [06:00<02:43,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|██████▉   | 94/135 [06:04<02:41,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  70%|███████   | 95/135 [06:08<02:33,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  71%|███████   | 96/135 [06:12<02:27,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  72%|███████▏  | 97/135 [06:16<02:31,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 98/135 [06:20<02:23,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  73%|███████▎  | 99/135 [06:24<02:20,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  74%|███████▍  | 100/135 [06:28<02:16,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  75%|███████▍  | 101/135 [06:31<02:13,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▌  | 102/135 [06:35<02:09,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  76%|███████▋  | 103/135 [06:39<02:03,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  77%|███████▋  | 104/135 [06:43<01:56,  3.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  78%|███████▊  | 105/135 [06:47<01:55,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▊  | 106/135 [06:51<01:52,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  79%|███████▉  | 107/135 [06:54<01:46,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  80%|████████  | 108/135 [06:58<01:40,  3.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████  | 109/135 [07:02<01:39,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  81%|████████▏ | 110/135 [07:06<01:36,  3.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  82%|████████▏ | 111/135 [07:10<01:33,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  83%|████████▎ | 112/135 [07:14<01:29,  3.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▎ | 113/135 [07:17<01:23,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  84%|████████▍ | 114/135 [07:21<01:20,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  85%|████████▌ | 115/135 [07:25<01:18,  3.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  86%|████████▌ | 116/135 [07:29<01:14,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 117/135 [07:33<01:08,  3.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  87%|████████▋ | 118/135 [07:37<01:04,  3.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  88%|████████▊ | 119/135 [07:40<01:00,  3.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  89%|████████▉ | 120/135 [07:45<00:59,  3.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|████████▉ | 121/135 [07:48<00:53,  3.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  90%|█████████ | 122/135 [07:52<00:48,  3.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  91%|█████████ | 123/135 [07:55<00:44,  3.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  92%|█████████▏| 124/135 [08:00<00:42,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 125/135 [08:04<00:39,  3.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  93%|█████████▎| 126/135 [08:07<00:34,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  94%|█████████▍| 127/135 [08:12<00:31,  3.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  95%|█████████▍| 128/135 [08:15<00:27,  3.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▌| 129/135 [08:19<00:22,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  96%|█████████▋| 130/135 [08:23<00:19,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  97%|█████████▋| 131/135 [08:27<00:15,  3.82s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  98%|█████████▊| 132/135 [08:30<00:11,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▊| 133/135 [08:34<00:07,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting:  99%|█████████▉| 134/135 [08:38<00:03,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Predicting: 100%|██████████| 135/135 [08:41<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS FOR SENTENCES_ALLAGREE\n",
      "============================================================\n",
      "Overall Accuracy: 0.993\n",
      "Negative Accuracy: 1.000\n",
      "Neutral Accuracy: 0.978\n",
      "Positive Accuracy: 1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00        45\n",
      "     Neutral       1.00      0.98      0.99        45\n",
      "    Positive       0.98      1.00      0.99        45\n",
      "\n",
      "    accuracy                           0.99       135\n",
      "   macro avg       0.99      0.99      0.99       135\n",
      "weighted avg       0.99      0.99      0.99       135\n",
      "\n",
      "\n",
      "Completed sentences_allagree\n",
      "Final accuracy: 0.993\n",
      "Improvement over baseline: 0.493\n",
      "\n",
      "Completed all 4 agreement levels!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, agreement_level in enumerate(agreement_levels):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING {agreement_level.upper()} ({i+1}/{len(agreement_levels)})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare dataset\n",
    "        df, sentiment_dist = prepare_dataset(agreement_level)\n",
    "        \n",
    "        # Step 2: Create splits\n",
    "        X_train, X_val, X_test = create_balanced_splits(df)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(f\"Skipping {agreement_level} - insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Load fresh model and tokenizer\n",
    "        print(\"\\nLoading fresh model and tokenizer...\")\n",
    "        model, tokenizer = load_model_and_tokenizer()\n",
    "        \n",
    "        # Step 4: Prepare prompts\n",
    "        train_data, eval_data, test_prompts, y_true = prepare_prompts(X_train, X_val, X_test, tokenizer)\n",
    "        \n",
    "        # Step 5: Test baseline performance (quick test on subset)\n",
    "        print(\"\\nTesting baseline performance...\")\n",
    "        test_subset_size = min(20, len(test_prompts))  # Small subset for baseline\n",
    "        test_subset = test_prompts.head(test_subset_size)\n",
    "        true_subset = y_true[:test_subset_size]\n",
    "        \n",
    "        baseline_predictions = predict_sentiment(test_subset, model, tokenizer)\n",
    "        baseline_accuracy = accuracy_score(\n",
    "            np.vectorize(lambda x: {'positive': 2, 'neutral': 1, 'none': 1, 'negative': 0}.get(x, 1))(true_subset),\n",
    "            np.vectorize(lambda x: {'positive': 2, 'neutral': 1, 'none': 1, 'negative': 0}.get(x, 1))(baseline_predictions)\n",
    "        )\n",
    "        print(f\"Baseline accuracy on {test_subset_size} samples: {baseline_accuracy:.3f}\")\n",
    "        \n",
    "        # Step 6: Fine-tune model\n",
    "        fine_tuned_model = fine_tune_model(model, tokenizer, train_data, eval_data, agreement_level)\n",
    "        \n",
    "        # Step 7: Evaluate fine-tuned model\n",
    "        print(f\"\\nEvaluating fine-tuned model on {len(test_prompts)} test samples...\")\n",
    "        final_predictions = predict_sentiment(test_prompts, fine_tuned_model, tokenizer)\n",
    "        \n",
    "        # Step 8: Calculate metrics\n",
    "        results = evaluate_model(y_true, final_predictions, agreement_level)\n",
    "        results['baseline_accuracy'] = baseline_accuracy\n",
    "        results['dataset_size'] = len(df)\n",
    "        results['train_size'] = len(train_data)\n",
    "        results['timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append(results)\n",
    "        detailed_results[agreement_level] = {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': final_predictions,\n",
    "            'test_data': X_test.copy()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCompleted {agreement_level}\")\n",
    "        print(f\"Final accuracy: {results['overall_accuracy']:.3f}\")\n",
    "        print(f\"Improvement over baseline: {results['overall_accuracy'] - baseline_accuracy:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {agreement_level}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    finally:\n",
    "        # Clean up memory after each model\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'tokenizer' in locals():\n",
    "            del tokenizer\n",
    "        if 'fine_tuned_model' in locals():\n",
    "            del fine_tuned_model\n",
    "        if 'trainer' in locals():\n",
    "            del trainer\n",
    "        \n",
    "        # Wait and clear memory between runs (except for the last one)\n",
    "        if i < len(agreement_levels) - 1:\n",
    "            wait_and_clear(60)\n",
    "\n",
    "print(f\"\\nCompleted all {len(all_results)} agreement levels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE RESULTS COMPARISON\n",
      "====================================================================================================\n",
      "      Agreement_Level  Dataset_Size  Train_Size  Test_Size  Baseline_Accuracy  \\\n",
      "0   sentences_50agree          4846         630        135               0.50   \n",
      "1   sentences_66agree          4217         630        135               0.40   \n",
      "2   sentences_75agree          3453         630        135               0.45   \n",
      "3  sentences_allagree          2264         630        135               0.50   \n",
      "\n",
      "   Final_Accuracy  Improvement  Improvement_Percent            Timestamp  \n",
      "0           0.874        0.374               74.815  2025-06-29 17:55:52  \n",
      "1           0.911        0.511              127.778  2025-06-29 20:04:12  \n",
      "2           0.963        0.513              113.992  2025-06-29 22:25:33  \n",
      "3           0.993        0.493               98.519  2025-06-30 03:32:31  \n",
      "\n",
      "Results saved to: ../results/llama3_agreement_levels_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive results DataFrame\n",
    "if all_results:\n",
    "    comparison_df = pd.DataFrame([\n",
    "        {\n",
    "            'Agreement_Level': result['agreement_level'],\n",
    "            'Dataset_Size': result['dataset_size'],\n",
    "            'Train_Size': result['train_size'],\n",
    "            'Test_Size': result['test_samples'],\n",
    "            'Baseline_Accuracy': result['baseline_accuracy'],\n",
    "            'Final_Accuracy': result['overall_accuracy'],\n",
    "            'Improvement': result['overall_accuracy'] - result['baseline_accuracy'],\n",
    "            'Improvement_Percent': ((result['overall_accuracy'] - result['baseline_accuracy']) / result['baseline_accuracy']) * 100,\n",
    "            'Timestamp': result['timestamp']\n",
    "        }\n",
    "        for result in all_results\n",
    "    ])\n",
    "    \n",
    "    # Sort by agreement level for better display\n",
    "    level_order = ['sentences_50agree', 'sentences_66agree', 'sentences_75agree', 'sentences_allagree']\n",
    "    comparison_df['Level_Order'] = comparison_df['Agreement_Level'].apply(lambda x: level_order.index(x) if x in level_order else 999)\n",
    "    comparison_df = comparison_df.sort_values('Level_Order').drop('Level_Order', axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Display results table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Save results\n",
    "    results_filename = \"../results/llama3_agreement_levels_comparison.csv\"\n",
    "    comparison_df.to_csv(results_filename, index=False)\n",
    "    print(f\"\\nResults saved to: {results_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results to display - all experiments failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS AND ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "BEST PERFORMING MODEL:\n",
      "  Agreement Level: sentences_allagree\n",
      "  Final Accuracy: 0.993\n",
      "  Dataset Size: 2,264\n",
      "  Improvement: 0.493 (+98.5%)\n",
      "\n",
      "LOWEST PERFORMING MODEL:\n",
      "  Agreement Level: sentences_50agree\n",
      "  Final Accuracy: 0.874\n",
      "  Dataset Size: 4,846\n",
      "  Improvement: 0.374 (+74.8%)\n",
      "\n",
      "TRENDS ANALYSIS:\n",
      "  Average Final Accuracy: 0.935\n",
      "  Average Improvement: 0.473\n",
      "  Standard Deviation: 0.053\n",
      "\n",
      "CORRELATION ANALYSIS:\n",
      "  Agreement Level vs Final Accuracy Correlation: 0.959\n",
      "  Strong positive correlation - Higher agreement improves performance\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "  • Use sentences_allagree for production deployment\n",
      "  • Data quality vs quantity trade-off analysis completed\n",
      "  • Consider ensemble methods if performance differences are small\n",
      "  • Monitor real-world performance to validate these results\n"
     ]
    }
   ],
   "source": [
    "# Analyze trends and insights\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS AND ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best and worst performing models\n",
    "    best_model = comparison_df.loc[comparison_df['Final_Accuracy'].idxmax()]\n",
    "    worst_model = comparison_df.loc[comparison_df['Final_Accuracy'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nBEST PERFORMING MODEL:\")\n",
    "    print(f\"  Agreement Level: {best_model['Agreement_Level']}\")\n",
    "    print(f\"  Final Accuracy: {best_model['Final_Accuracy']:.3f}\")\n",
    "    print(f\"  Dataset Size: {best_model['Dataset_Size']:,}\")\n",
    "    print(f\"  Improvement: {best_model['Improvement']:.3f} ({best_model['Improvement_Percent']:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nLOWEST PERFORMING MODEL:\")\n",
    "    print(f\"  Agreement Level: {worst_model['Agreement_Level']}\")\n",
    "    print(f\"  Final Accuracy: {worst_model['Final_Accuracy']:.3f}\")\n",
    "    print(f\"  Dataset Size: {worst_model['Dataset_Size']:,}\")\n",
    "    print(f\"  Improvement: {worst_model['Improvement']:.3f} ({worst_model['Improvement_Percent']:+.1f}%)\")\n",
    "    \n",
    "    # Analyze relationship between agreement level and performance\n",
    "    print(f\"\\nTRENDS ANALYSIS:\")\n",
    "    print(f\"  Average Final Accuracy: {comparison_df['Final_Accuracy'].mean():.3f}\")\n",
    "    print(f\"  Average Improvement: {comparison_df['Improvement'].mean():.3f}\")\n",
    "    print(f\"  Standard Deviation: {comparison_df['Final_Accuracy'].std():.3f}\")\n",
    "    \n",
    "    # Check if higher agreement correlates with better performance\n",
    "    agreement_mapping = {\n",
    "        'sentences_50agree': 50,\n",
    "        'sentences_66agree': 66,\n",
    "        'sentences_75agree': 75,\n",
    "        'sentences_allagree': 100\n",
    "    }\n",
    "    \n",
    "    comparison_df['Agreement_Percent'] = comparison_df['Agreement_Level'].map(agreement_mapping)\n",
    "    correlation = comparison_df['Agreement_Percent'].corr(comparison_df['Final_Accuracy'])\n",
    "    \n",
    "    print(f\"\\nCORRELATION ANALYSIS:\")\n",
    "    print(f\"  Agreement Level vs Final Accuracy Correlation: {correlation:.3f}\")\n",
    "    \n",
    "    if correlation > 0.5:\n",
    "        print(f\"  Strong positive correlation - Higher agreement improves performance\")\n",
    "    elif correlation > 0.2:\n",
    "        print(f\"  Moderate positive correlation - Higher agreement tends to improve performance\")\n",
    "    elif correlation > -0.2:\n",
    "        print(f\"  Weak correlation - Agreement level has minimal impact on performance\")\n",
    "    else:\n",
    "        print(f\"  Negative correlation - Unexpected result, may need investigation\")\n",
    "    \n",
    "    print(f\"\\nRECOMMENDATIONS:\")\n",
    "    print(f\"  • Use {best_model['Agreement_Level']} for production deployment\")\n",
    "    print(f\"  • Data quality vs quantity trade-off analysis completed\")\n",
    "    print(f\"  • Consider ensemble methods if performance differences are small\")\n",
    "    print(f\"  • Monitor real-world performance to validate these results\")\n",
    "    \n",
    "else:\n",
    "    print(\"No analysis possible - no successful experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed predictions saved: ../results/detailed_predictions_llama3_sentences_50agree.csv\n",
      "Detailed predictions saved: ../results/detailed_predictions_llama3_sentences_66agree.csv\n",
      "Detailed predictions saved: ../results/detailed_predictions_llama3_sentences_75agree.csv\n",
      "Detailed predictions saved: ../results/detailed_predictions_llama3_sentences_allagree.csv\n",
      "\n",
      "All results and analysis completed!\n",
      "Check the ../results/ folder for all output files\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results for each agreement level\n",
    "if detailed_results:\n",
    "    for agreement_level, data in detailed_results.items():\n",
    "        # Create detailed predictions DataFrame\n",
    "        detailed_df = pd.DataFrame({\n",
    "            'text': data['test_data']['text'].tolist(),\n",
    "            'true_sentiment': data['y_true'],\n",
    "            'predicted_sentiment': data['y_pred'],\n",
    "            'correct': [t == p for t, p in zip(data['y_true'], data['y_pred'])],\n",
    "            'agreement_level': agreement_level\n",
    "        })\n",
    "        \n",
    "        # Save detailed results\n",
    "        detail_filename = f\"../results/detailed_predictions_llama3_{agreement_level}.csv\"\n",
    "        detailed_df.to_csv(detail_filename, index=False)\n",
    "        print(f\"Detailed predictions saved: {detail_filename}\")\n",
    "\n",
    "print(f\"\\nAll results and analysis completed!\")\n",
    "print(f\"Check the ../results/ folder for all output files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing memory...\n",
      "GPU memory allocated: 0.02 GB\n",
      "GPU memory reserved: 1.04 GB\n",
      "\n",
      "Final memory cleanup completed\n",
      "\n",
      "Experiment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Final memory cleanup\n",
    "clear_memory()\n",
    "print(\"\\nFinal memory cleanup completed\")\n",
    "print(\"\\nExperiment completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
